Directory structure:
└── waterlogging-prediction/
    ├── README.md
    ├── Deployment.md
    ├── Dockerfile
    ├── api-doc.md
    ├── business-prompt.md
    ├── config.py
    ├── log.txt
    ├── requirements.txt
    ├── setup.sh
    ├── submission.md
    ├── train.py
    ├── api/
    │   ├── __init__.py
    │   ├── ai-server.py
    │   ├── app.py
    │   ├── auth_routes.py
    │   ├── authority_routes.py
    │   ├── digest.txt
    │   ├── forum_routes.py
    │   ├── model_routes.py
    │   ├── route_routes.py
    │   ├── weather_routes.py
    │   └── __pycache__/
    ├── data/
    ├── db/
    ├── models/
    │   ├── data_processor.py
    │   ├── feature_engineer.py
    │   ├── performance_analytics.py
    │   ├── risk_config.joblib
    │   ├── risk_predictor.py
    │   ├── waterlogging_model.joblib
    │   ├── waterlogging_predictor.py
    │   └── __pycache__/
    └── utils/

================================================
File: README.md
================================================
# Waterlogging Prediction and Risk Analysis System

This system predicts waterlogging depth and analyzes risk based on rainfall data and geographical features.

## Project Structure

```
waterlogging_prediction/
│
├── data/                   # CSV data files for stations
│   ├── 1.csv
│   ├── 2.csv
│   └── ...
│
├── models/                 # Model implementation and saved models
│   ├── __init__.py
│   ├── data_processor.py
│   ├── feature_engineer.py
│   ├── waterlogging_predictor.py
│   ├── risk_predictor.py
│   ├── waterlogging_model.joblib  # Saved model (after training)
│   └── risk_config.joblib         # Saved risk config (after training)
│
├── utils/                  # Utility functions
│   └── __init__.py
│
├── api/                    # Flask API implementation
│   ├── __init__.py
│   └── app.py
    └── ...
│
├── config.py               # Configuration settings
├── train.py                # Training script
├── requirements.txt        # Dependencies
└── README.md               # This file
```

## Features

1. **Data Processing**: 
   - Handles missing values
   - Interpolates time series data
   - Creates sliding windows for time series modeling

2. **Feature Engineering**:
   - Extracts statistical features from rainfall time series
   - Constructs domain-specific features (seasonality, rainfall interval, wetting coefficient, etc.)
   - Incorporates weather and wind speed data when available

3. **Machine Learning Models**:
   - Random Forest (default)
   - Gradient Boosting Decision Trees
   - AdaBoost

4. **Risk Analysis**:
   - Calculates amplification factors
   - Incorporates geographical features
   - Produces risk scores and levels (low, moderate, high)



## Setup Instructions

### Prerequisites

- Python 3.8+
- pip (Python package manager)
- Virtual environment (recommended)

### Installation

1. Clone the repository:

```bash
git clone https://github.com/yourusername/waterlogging_prediction.git
cd waterlogging_prediction
```

2. Create and activate a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install the required packages:

```bash
pip install -r requirements.txt
```

### Data Preparation

Place your CSV data files in the `data/` directory. Each file should correspond to one station, with the filename format `<station_id>.csv`.

Each CSV should contain at least the following columns:
- `clctTime`: Timestamp
- `Rainfall(mm)`: Rainfall in millimeters
- `Waterdepth(meters)`: Water depth in meters

Optional columns:
- `Weather`: Weather condition code
- `Wind Speed`: Wind speed

### Model Training

To train the model using the default settings:

```bash
python train.py --data-dir data --model-type rf --window-size 6 --save-dir models
```

Options:
- `--data-dir`: Directory containing CSV files (default: 'data')
- `--model-type`: Type of model ('rf', 'gbdt', 'adaboost') (default: 'rf')
- `--window-size`: Size of sliding window (default: 6)
- `--tune-hyperparams`: Flag to tune hyperparameters
- `--save-dir`: Directory to save models (default: 'models')

### Running the API

To start the Flask API:

```bash
cd api
python app.py
```

The API will be available at `http://localhost:5000/`.

## API Endpoints

- [Find the api doc here](api-doc.md)

## Implementation Details

The system follows the approach from the research paper with these key components:

1. **Data Preprocessing**:
   - Interpolation for missing timestamps
   - Handling missing values
   - Time series slicing

2. **Feature Extraction**:
   - Unit rainfall calculation
   - Seasonality coefficients
   - Rainfall interval features
   - Statistical features (mean, max, std, kurtosis, skewness, AUC)

3. **Risk Analysis**:
   - Amplification factor (AF) = waterlogging depth / rainfall
   - Risk formula: r = w1*AF + w2*(1-elevation_normalized) + w3*impervious_cover + w4*(1-drainage_normalized) + w5*(1-slope_normalized) + w6*proximity_norm
   - Risk levels: low (<0.3), moderate (0.3-0.6), high (>0.6)

4. **Machine Learning Pipeline**:
   - Data processing
   - Feature engineering
   - Model training and prediction

5. **Feedback Loop**:
   - Continuous model improvement with new data
   - Updating amplification factors

## Requirements

The following packages are required:

- Flask==2.0.1
- pandas==1.3.3
- numpy==1.20.3
- scikit-learn==0.24.2
- joblib==1.0.1

To install all requirements:

```bash
pip install -r requirements.txt
```


================================================
File: Deployment.md
================================================
# Deployment Guide

This guide provides instructions for deploying the Waterlogging Prediction System in various environments.

## Local Deployment

### Prerequisites
- Python 3.8+
- pip package manager
- Git (optional)

### Steps

1. Clone or download the repository:
```bash
git clone https://github.com/yourusername/waterlogging_prediction.git
cd waterlogging_prediction
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Train the model:
```bash
python train.py --data-dir data --model-type rf
```

5. Run the Flask API:
```bash
cd api
python app.py
```

The API will be available at `http://localhost:5000/`.

## Docker Deployment

### Prerequisites
- Docker
- Docker Compose (optional)

### Using Dockerfile

1. Build the Docker image:
```bash
docker build -t waterlogging-prediction:latest .
```

2. Run the container:
```bash
docker run -p 5000:5000 -v $(pwd)/data:/app/data -v $(pwd)/models:/app/models waterlogging-prediction:latest
```

This will map the container's port 5000 to the host's port 5000 and mount the local data and models directories to the container.

### Using Docker Compose

Create a `docker-compose.yml` file:

```yaml
version: '3'
services:
  app:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - FLASK_ENV=production
```

Run the application:
```bash
docker-compose up -d
```

## Production Deployment

For production environments, consider the following recommendations:

### Using Gunicorn

Gunicorn is a production-ready WSGI server for Python web applications.

1. Install Gunicorn (already included in requirements.txt):
```bash
pip install gunicorn
```

2. Run the application with Gunicorn:
```bash
gunicorn --bind 0.0.0.0:5000 api.app:app
```

### Using Nginx as a Reverse Proxy

For improved performance and security, use Nginx as a reverse proxy in front of Gunicorn.

1. Install Nginx:
```bash
sudo apt-get install nginx  # Ubuntu/Debian
# or
sudo yum install nginx      # CentOS/RHEL
```

2. Create an Nginx configuration file:
```
server {
    listen 80;
    server_name your_domain.com;

    location / {
        proxy_pass http://localhost:5000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

3. Enable the configuration and restart Nginx:
```bash
sudo ln -s /etc/nginx/sites-available/your_config /etc/nginx/sites-enabled/
sudo systemctl restart nginx
```

### Using Supervisor to Manage the Process

Supervisor can keep your application running and automatically restart it if it crashes.

1. Install Supervisor:
```bash
sudo apt-get install supervisor  # Ubuntu/Debian
# or
sudo yum install supervisor      # CentOS/RHEL
```

2. Create a configuration file:
```
[program:waterlogging-prediction]
command=/path/to/venv/bin/gunicorn --bind 0.0.0.0:5000 api.app:app
directory=/path/to/waterlogging_prediction
user=your_user
autostart=true
autorestart=true
stderr_logfile=/var/log/waterlogging_prediction.err.log
stdout_logfile=/var/log/waterlogging_prediction.out.log
```

3. Enable the configuration:
```bash
sudo supervisorctl reread
sudo supervisorctl update
sudo supervisorctl start waterlogging-prediction
```

## Cloud Deployment

### AWS Elastic Beanstalk

1. Install the EB CLI:
```bash
pip install awsebcli
```

2. Initialize your EB project:
```bash
eb init -p python-3.8 waterlogging-prediction
```

3. Create an environment and deploy:
```bash
eb create waterlogging-prediction-env
```

4. For subsequent deployments:
```bash
eb deploy
```

### Google Cloud Run

1. Install the Google Cloud SDK.

2. Build and push the Docker image:
```bash
gcloud builds submit --tag gcr.io/your-project-id/waterlogging-prediction
```

3. Deploy to Cloud Run:
```bash
gcloud run deploy waterlogging-prediction \
  --image gcr.io/your-project-id/waterlogging-prediction \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

### Azure App Service

1. Install the Azure CLI.

2. Create an App Service Plan:
```bash
az appservice plan create --name waterlogging-prediction-plan --resource-group your-resource-group --sku B1
```

3. Create a Web App:
```bash
az webapp create --name waterlogging-prediction --resource-group your-resource-group --plan waterlogging-prediction-plan --runtime "PYTHON|3.8"
```

4. Deploy the application:
```bash
az webapp up --name waterlogging-prediction --resource-group your-resource-group
```

## Monitoring and Maintenance

### Logging

The application uses Python's built-in logging module. Logs are written to:
- Console output
- `app.log` file
- `training.log` file (for model training)

### Backup

Regularly backup your model files:
- `models/waterlogging_model.joblib`
- `models/risk_config.joblib`

### Updating the Model

To update the model with new data:
1. Add new CSV files to the `data/` directory
2. Run the training script:
```bash
python train.py --data-dir data --model-type rf
```

Alternatively, use the `/model/feedback` API endpoint to continuously improve the model with new observations.

### Security Considerations

1. Use HTTPS in production with a valid SSL certificate
2. Implement proper authentication for the API
3. Consider rate limiting to prevent abuse
4. Regularly update dependencies to address security vulnerabilities


================================================
File: Dockerfile
================================================
# Use Python 3.8 as base image
FROM python:3.9-alpine

# Set working directory
WORKDIR /app

# Copy requirements file
COPY requirements.txt .

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create data directory if it doesn't exist
RUN mkdir -p data models

# Expose port
EXPOSE 5000

# Set environment variables
ENV PYTHONPATH=/app
ENV FLASK_APP=api/app.py
ENV FLASK_ENV=production

# Start the application
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "api.app:app"]


================================================
File: api-doc.md
================================================
## API Documentation

### 1. Model Prediction Endpoints

#### 1.1 Predict Waterlogging Depth and Risk

**Endpoint**: `POST /model/predict/`

**Request Body**:
```json
{
    "station_code": "1",
    "rainfall": 10.5,
    "timestamp": "2023-05-01T12:30:00",
    "weather": 2,  
    "windspeed": 5.2  
}
```

**Response**:
```json
{
    "prediction": {
        "waterlogging_depth": 0.25,
        "risk_factor": {
            "risk_score": 0.45,
            "risk_level": "moderate",
            "amplification_factor": 0.23,
            "factors": {
                "elevation": 0.3,
                "impervious_cover": 0.8,
                "drainage": 0.4,
                "slope": 0.2,
                "proximity_to_water": 0.5
            }
        }
    },
    "message": "Prediction successful",
    "status": "success"
}
```

#### 1.2 Provide Feedback

**Endpoint**: `POST /model/feedback`

**Request Body**:
```json
{
    "station_code": "1",
    "rainfall": 10.5,
    "timestamp": "2023-05-01T12:30:00",
    "weather": 2,
    "windspeed": 5.2,
    "actual_waterdepth": 0.27
}
```

**Response**:
```json
{
    "message": "Feedback processed successfully",
    "status": "success",
    "details": {
        "previous_prediction": 0.25,
        "actual_value": 0.27,
        "error": 0.02
    }
}
```

#### 1.3 Get or Update Risk Weights

**Endpoint**: `GET /model/weights`

**Response**:
```json
{
    "weights": {
        "amplification_factor": 0.4,
        "elevation": 0.2,
        "impervious_cover": 0.1,
        "drainage": 0.15,
        "slope": 0.1,
        "proximity_to_water": 0.05
    },
    "message": "Current weights retrieved successfully",
    "status": "success"
}
```

**Endpoint**: `POST /model/weights`

**Request Body**:
```json
{
    "weights": {
        "amplification_factor": 0.5,
        "elevation": 0.2,
        "impervious_cover": 0.1,
        "drainage": 0.1,
        "slope": 0.05,
        "proximity_to_water": 0.05
    }
}
```

**Response**: Same as GET response but with updated weights.

#### 1.4 Get or Update Station Data

**Endpoint**: `GET /model/station-data?station_id=1`

**Response**:
```json
{
    "station_data": {
        "amplification_factor": 0.23,
        "elevation": 25.5,
        "impervious_cover": 0.85,
        "drainage_area": 200,
        "drainage_volume": 10000,
        "slope": 0.05,
        "proximity_to_water": 300
    },
    "message": "Station data retrieved successfully",
    "status": "success"
}
```

**Endpoint**: `POST /model/station-data`

**Request Body**:
```json
{
    "station_id": "1",
    "data": {
        "elevation": 25.5,
        "impervious_cover": 0.85,
        "drainage_area": 200,
        "drainage_volume": 10000,
        "slope": 0.05,
        "proximity_to_water": 300
    }
}
```

**Response**: Same as GET response but with updated station data.

### 2. Weather Data Endpoints

#### 2.1 Get Current Weather Data

**Endpoint**: `GET /weather/current?lat=23.8103&lon=90.4125`

**Response**:
```json
{
    "location": "Dhaka, Bangladesh",
    "currentCondition": "Heavy Rain",
    "temperature": 28,
    "rainfall": {
        "intensity": "heavy",
        "rate": 15.2
    },
    "timestamp": "2023-05-01T12:30:00Z",
    "message": "Weather data retrieved successfully",
    "status": "success"
}
```

#### 2.2 Get Weather Forecast

**Endpoint**: `GET /weather/forecast?lat=23.8103&lon=90.4125`

**Response**:
```json
{
    "location": "Dhaka, Bangladesh",
    "forecast": [
        {
            "time": "1h",
            "condition": "Heavy Rain",
            "temperature": 28,
            "rainfall": {
                "intensity": "heavy",
                "rate": 18.5
            }
        },
        {
            "time": "2h",
            "condition": "Moderate Rain",
            "temperature": 27,
            "rainfall": {
                "intensity": "medium",
                "rate": 8.2
            }
        },
        {
            "time": "3h",
            "condition": "Light Rain",
            "temperature": 26,
            "rainfall": {
                "intensity": "light",
                "rate": 3.1
            }
        }
    ],
    "message": "Weather forecast retrieved successfully",
    "status": "success"
}
```

#### 2.3 Get Weather Alerts

**Endpoint**: `GET /weather/alerts?lat=23.8103&lon=90.4125`

**Response**:
```json
{
    "location": "Dhaka, Bangladesh",
    "alerts": [
        {
            "type": "Flash waterlogging",
            "severity": "danger",
            "title": "Flash waterlogging Warning",
            "description": "Flash waterlogging is occurring or imminent in the warned area. Move to higher ground immediately.",
            "issued": "2023-05-01T10:30:00Z",
            "expires": "2023-05-01T16:30:00Z"
        }
    ],
    "message": "Weather alerts retrieved successfully",
    "status": "success"
}
```

### 3. Route Planning Endpoints

#### 3.1 Get Safe Routes

**Endpoint**: `POST /routes/plan`

**Request Body**:
```json
{
    "startLocation": "Dhaka",
    "endLocation": "Khulna",
    "timestamp": "2023-05-01T12:30:00"
}
```

**Response**:
```json
{
    "routes": [
        {
            "id": "route-1",
            "name": "Safest Route",
            "startLocation": {
                "lat": 23.8103,
                "lon": 90.4125,
                "name": "Dhaka"
            },
            "endLocation": {
                "lat": 22.8456,
                "lon": 89.5403,
                "name": "Khulna"
            },
            "segments": [
                {
                    "startPoint": {
                        "lat": 23.8103,
                        "lon": 90.4125
                    },
                    "endPoint": {
                        "lat": 23.3280,
                        "lon": 90.1764
                    },
                    "distance": 62000,
                    "duration": 4464,
                    "floodRisk": "none",
                    "roadType": "highway"
                },
                {
                    "startPoint": {
                        "lat": 23.3280,
                        "lon": 90.1764
                    },
                    "endPoint": {
                        "lat": 22.8456,
                        "lon": 89.5403
                    },
                    "distance": 248000,
                    "duration": 17856,
                    "floodRisk": "low",
                    "roadType": "major"
                }
            ],
            "totalDistance": 310000,
            "totalDuration": 22320,
            "safetyScore": 92,
            "safetyIssues": []
        },
        {
            "id": "route-2",
            "name": "Balanced Route",
            "startLocation": {
                "lat": 23.8103,
                "lon": 90.4125,
                "name": "Dhaka"
            },
            "endLocation": {
                "lat": 22.8456,
                "lon": 89.5403,
                "name": "Khulna"
            },
            "segments": [
                {
                    "startPoint": {
                        "lat": 23.8103,
                        "lon": 90.4125
                    },
                    "endPoint": {
                        "lat": 23.1640,
                        "lon": 89.9883
                    },
                    "distance": 90000,
                    "duration": 6480,
                    "floodRisk": "low",
                    "roadType": "highway"
                },
                {
                    "startPoint": {
                        "lat": 23.1640,
                        "lon": 89.9883
                    },
                    "endPoint": {
                        "lat": 22.8456,
                        "lon": 89.5403
                    },
                    "distance": 180000,
                    "duration": 12960,
                    "floodRisk": "medium",
                    "roadType": "major"
                }
            ],
            "totalDistance": 270000,
            "totalDuration": 19440,
            "safetyScore": 75,
            "safetyIssues": [
                {
                    "type": "waterlogging",
                    "description": "Moderate waterlogging reported near Faridpur",
                    "severity": "warning",
                    "location": {
                        "lat": 23.6065,
                        "lon": 89.8447
                    }
                }
            ]
        },
        {
            "id": "route-3",
            "name": "Shortest Route",
            "startLocation": {
                "lat": 23.8103,
                "lon": 90.4125,
                "name": "Dhaka"
            },
            "endLocation": {
                "lat": 22.8456,
                "lon": 89.5403,
                "name": "Khulna"
            },
            "segments": [
                {
                    "startPoint": {
                        "lat": 23.8103,
                        "lon": 90.4125
                    },
                    "endPoint": {
                        "lat": 23.4657,
                        "lon": 89.9764
                    },
                    "distance": 60000,
                    "duration": 4320,
                    "floodRisk": "medium",
                    "roadType": "highway"
                },
                {
                    "startPoint": {
                        "lat": 23.4657,
                        "lon": 89.9764
                    },
                    "endPoint": {
                        "lat": 23.0000,
                        "lon": 89.7532
                    },
                    "distance": 70000,
                    "duration": 5040,
                    "floodRisk": "high",
                    "roadType": "local"
                },
                {
                    "startPoint": {
                        "lat": 23.0000,
                        "lon": 89.7532
                    },
                    "endPoint": {
                        "lat": 22.8456,
                        "lon": 89.5403
                    },
                    "distance": 113000,
                    "duration": 8136,
                    "floodRisk": "extreme",
                    "roadType": "local"
                }
            ],
            "totalDistance": 243000,
            "totalDuration": 17496,
            "safetyScore": 45,
            "safetyIssues": [
                {
                    "type": "waterlogging",
                    "description": "Severe waterlogging reported on Jessore Highway",
                    "severity": "danger",
                    "location": {
                        "lat": 23.4657,
                        "lon": 89.9764
                    }
                },
                {
                    "type": "closure",
                    "description": "Road closure near Magura due to water levels",
                    "severity": "danger",
                    "location": {
                        "lat": 23.0000,
                        "lon": 89.7532
                    }
                }
            ]
        }
    ],
    "message": "Routes retrieved successfully",
    "status": "success"
}
```

### 4. Forum Endpoints

#### 4.1 Get All Posts

**Endpoint**: `GET /forum/posts?location=Dhaka&limit=10&offset=0`

**Response**:
```json
{
    "posts": [
        {
            "id": "post-1",
            "title": "waterlogging in Mirpur Area",
            "content": "Heavy waterlogging reported in parts of Mirpur. The main road is under 0.5m of water.",
            "location": "Mirpur, Dhaka",
            "created_at": "2023-05-01T10:30:00Z",
            "updated_at": "2023-05-01T10:30:00Z",
            "user_id": "user-1",
            "upvotes": 12,
            "downvotes": 2,
            "images": [
                {
                    "id": "img-1",
                    "image_url": "https://example.com/images/flood1.jpg"
                }
            ],
            "user_vote": null
        }
    ],
    "total": 1,
    "message": "Posts retrieved successfully",
    "status": "success"
}
```

#### 4.2 Create Post

**Endpoint**: `POST /forum/posts`

**Request Body**:
```json
{
    "title": "waterlogging in Mirpur Area",
    "content": "Heavy waterlogging reported in parts of Mirpur. The main road is under 0.5m of water.",
    "location": "Mirpur, Dhaka",
    "user_id": "user-1"
}
```

**Response**:
```json
{
    "post": {
        "id": "post-1",
        "title": "waterlogging in Mirpur Area",
        "content": "Heavy waterlogging reported in parts of Mirpur. The main road is under 0.5m of water.",
        "location": "Mirpur, Dhaka",
        "created_at": "2023-05-01T10:30:00Z",
        "updated_at": "2023-05-01T10:30:00Z",
        "user_id": "user-1",
        "upvotes": 0,
        "downvotes": 0
    },
    "message": "Post created successfully",
    "status": "success"
}
```

#### 4.3 Upload Post Image

**Endpoint**: `POST /forum/posts/:post_id/images`

**Request Body**: Multipart form data with image file

**Response**:
```json
{
    "image": {
        "id": "img-1",
        "post_id": "post-1",
        "image_url": "https://example.com/images/flood1.jpg",
        "created_at": "2023-05-01T10:35:00Z"
    },
    "message": "Image uploaded successfully",
    "status": "success"
}
```

#### 4.4 Vote on Post

**Endpoint**: `POST /forum/posts/:post_id/vote`

**Request Body**:
```json
{
    "user_id": "user-2",
    "vote_type": "upvote"
}
```

**Response**:
```json
{
    "post": {
        "id": "post-1",
        "upvotes": 13,
        "downvotes": 2,
        "user_vote": "upvote"
    },
    "message": "Vote recorded successfully",
    "status": "success"
}
```

### 5. Authentication Endpoints

#### 5.1 Sign Up

**Endpoint**: `POST /auth/signup`

**Request Body**:
```json
{
    "email": "user@example.com",
    "password": "securepassword"
}
```

**Response**:
```json
{
    "user": {
        "id": "user-3",
        "email": "user@example.com",
        "created_at": "2023-05-01T12:00:00Z"
    },
    "message": "User created successfully",
    "status": "success"
}
```

#### 5.2 Sign In

**Endpoint**: `POST /auth/signin`

**Request Body**:
```json
{
    "email": "user@example.com",
    "password": "securepassword"
}
```

**Response**:
```json
{
    "user": {
        "id": "user-3",
        "email": "user@example.com"
    },
    "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
    "message": "Signed in successfully",
    "status": "success"
}
```

### 6. Authority Dashboard Endpoints

#### 6.1 Get All Stations

**Endpoint**: `GET /authority/stations`

**Response**:
```json
{
    "stations": [
        {
            "id": 1,
            "name": "Station A",
            "lat": 23.8103,
            "lon": 90.4125,
            "details": {
                "elevation": "11.2m",
                "landCover": "Urban/Impervious",
                "drainage": "Low",
                "slope": "1.8%",
                "proximity": "400m to Dhanmondi Lake"
            }
        },
        {
            "id": 2,
            "name": "Station B",
            "lat": 23.7000,
            "lon": 90.3750,
            "details": {
                "elevation": "9.6m",
                "landCover": "Mixed Residential",
                "drainage": "Moderate",
                "slope": "2.5%",
                "proximity": "700m to Buriganga River"
            }
        }
    ],
    "message": "Stations retrieved successfully",
    "status": "success"
}
```

#### 6.2 Get Station Data

**Endpoint**: `GET /authority/stations/:station_id/data?hours=3`

**Response**:
```json
{
    "station": {
        "id": 1,
        "name": "Station A",
        "lat": 23.8103,
        "lon": 90.4125
    },
    "data": [
        {
            "timestamp": "06:00",
            "waterlogging": 1.3,
            "rainfall": 10.0,
            "riskfactor": 2.0
        },
        {
            "timestamp": "06:30",
            "waterlogging": 1.2,
            "rainfall": 9.5,
            "riskfactor": 1.8
        },
        {
            "timestamp": "07:00",
            "waterlogging": 1.1,
            "rainfall": 9.0,
            "riskfactor": 1.7
        }
    ],
    "feedback": [
        {
            "user": "Hasan",
            "comment": "Water levels rise quickly here after heavy rain."
        },
        {
            "user": "Farzana",
            "comment": "Drainage improvements are working recently."
        }
    ],
    "message": "Station data retrieved successfully",
    "status": "success"
}
```

#### 6.3 Update Station Location

**Endpoint**: `POST /authority/stations/:station_id/location`

**Request Body**:
```json
{
    "lat": 23.8150,
    "lon": 90.4175
}
```

**Response**:
```json
{
    "station": {
        "id": 1,
        "name": "Station A",
        "lat": 23.8150,
        "lon": 90.4175
    },
    "message": "Station location updated successfully",
    "status": "success"
}
```


================================================
File: business-prompt.md
================================================



================================================
File: config.py
================================================
"""
Configuration settings for the waterlogging prediction system
"""

import os

# Base directory of the project
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Data directory
DATA_DIR = os.path.join(BASE_DIR, 'data')

# Models directory
MODELS_DIR = os.path.join(BASE_DIR, 'models')

# Model files
WATERLOGGING_MODEL_PATH = os.path.join(MODELS_DIR, 'waterlogging_model.joblib')
RISK_CONFIG_PATH = os.path.join(MODELS_DIR, 'risk_config.joblib')

# Default model parameters
DEFAULT_MODEL_TYPE = 'rf'  # Random Forest
DEFAULT_WINDOW_SIZE = 6
DEFAULT_SEASONALITY = {
    'rainy_months': [6, 7, 8, 9],  # June to September
    'transition_months': [3, 4, 5, 10],  # March to May, October
    'dry_months': [1, 2, 11, 12],  # November to February
    'coefficients': {
        'rainy': 10,      # Strong coefficient for rainy season
        'transition': 6,  # Medium coefficient for transition season
        'dry': 2          # Low coefficient for dry season
    }
}

# Default risk predictor weights
DEFAULT_RISK_WEIGHTS = {
    'amplification_factor': 0.4,
    'elevation': 0.2,
    'impervious_cover': 0.1,
    'drainage': 0.15,
    'slope': 0.1,
    'proximity_to_water': 0.05
}

# Risk levels
RISK_LEVELS = {
    'low': {'min': 0, 'max': 0.3},
    'moderate': {'min': 0.3, 'max': 0.6},
    'high': {'min': 0.6, 'max': 1.0}
}

# Flask API settings
API_HOST = '0.0.0.0'
API_PORT = 5000
API_DEBUG = False  # Set to False in production

# Logging configuration
LOG_LEVEL = 'INFO'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
LOG_FILE = os.path.join(BASE_DIR, 'app.log')

# Dummy geo-spatial data for stations
# This would be replaced with real data in a production environment
DUMMY_STATION_DATA = {
    '1': {
        'elevation': 33,
        'impervious_cover': 0.9,
        'drainage_area': 145.46,
        'drainage_volume': 7210.38,
        'slope': 0.04,
        'proximity_to_water': 300
    },
    '2': {
        'elevation': 18,
        'impervious_cover': 0.85,
        'drainage_area': 236.93,
        'drainage_volume': 9448.44,
        'slope': 0.06,
        'proximity_to_water': 250
    },
    '3': {
        'elevation': 5,
        'impervious_cover': 0.88,
        'drainage_area': 198.45,
        'drainage_volume': 14863.07,
        'slope': 0.03,
        'proximity_to_water': 180
    },
    '4': {
        'elevation': 11,
        'impervious_cover': 0.92,
        'drainage_area': 335.14,
        'drainage_volume': 11961.56,
        'slope': 0.05,
        'proximity_to_water': 220
    },
    '5': {
        'elevation': 25,
        'impervious_cover': 0.8,
        'drainage_area': 220.5,
        'drainage_volume': 9800.75,
        'slope': 0.045,
        'proximity_to_water': 280
    },
    '6': {
        'elevation': 15,
        'impervious_cover': 0.87,
        'drainage_area': 195.8,
        'drainage_volume': 8500.3,
        'slope': 0.035,
        'proximity_to_water': 320
    },
    '7': {
        'elevation': 28,
        'impervious_cover': 0.83,
        'drainage_area': 275.6,
        'drainage_volume': 10450.2,
        'slope': 0.055,
        'proximity_to_water': 190
    }
}


================================================
File: log.txt
================================================



================================================
File: requirements.txt
================================================
Flask>=2.0.0
pandas>=2.0.0
numpy>=1.20.0
scikit-learn>=1.0.0
joblib>=1.0.0
matplotlib>=3.0.0
seaborn>=0.11.0
gunicorn>=20.0.0
pytest>=6.0.0
python-dateutil>=2.8.0


================================================
File: setup.sh
================================================
#!/bin/bash

# Waterlogging Prediction System Setup Script
# This script sets up the environment, trains the model, and starts the API

# Exit on any error
set -e

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Print colored message
function print_message() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

function print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

function print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if Python is installed
if ! command -v python3 &> /dev/null; then
    print_error "Python 3 is not installed. Please install Python 3.8 or higher."
    exit 1
fi

# Check Python version
PYTHON_VERSION=$(python3 --version | cut -d " " -f 2)
PYTHON_MAJOR=$(echo $PYTHON_VERSION | cut -d "." -f 1)
PYTHON_MINOR=$(echo $PYTHON_VERSION | cut -d "." -f 2)

if [ "$PYTHON_MAJOR" -lt 3 ] || ([ "$PYTHON_MAJOR" -eq 3 ] && [ "$PYTHON_MINOR" -lt 8 ]); then
    print_error "Python version $PYTHON_VERSION is not supported. Please install Python 3.8 or higher."
    exit 1
fi

print_message "Using Python $PYTHON_VERSION"

# Create virtual environment
if [ ! -d "venv" ]; then
    print_message "Creating virtual environment..."
    python3 -m venv venv
else
    print_warning "Virtual environment already exists. Skipping creation."
fi

# Activate virtual environment
print_message "Activating virtual environment..."
source venv/bin/activate

# Install dependencies
print_message "Installing dependencies..."
pip install --upgrade pip
pip install -r requirements.txt

# Create directories if they don't exist
print_message "Setting up directories..."
mkdir -p data
mkdir -p models

# Check if any CSV files exist in the data directory
CSV_COUNT=$(find data -name "*.csv" | wc -l)
if [ "$CSV_COUNT" -eq 0 ]; then
    print_warning "No CSV files found in the data directory. Please add your data files to the 'data' directory."
    exit 1
fi

# Train the model
print_message "Training the model..."
python train.py --data-dir data --model-type rf

# Check if model files were created
if [ ! -f "models/waterlogging_model.joblib" ] || [ ! -f "models/risk_config.joblib" ]; then
    print_error "Model training failed. Model files not found."
    exit 1
fi

print_message "Model trained successfully!"

# Start the API (in the background)
print_message "Starting the API server..."
cd api
python app.py &
API_PID=$!

# Wait for the API to start
sleep 3

# Check if the API is running
if curl -s http://localhost:5000/model/weights > /dev/null; then
    print_message "API server is running at http://localhost:5000/"
    print_message "Press Ctrl+C to stop the server."
    
    # Wait for user to press Ctrl+C
    trap "kill $API_PID; print_message 'API server stopped.'; exit 0" INT
    wait $API_PID
else
    print_error "API server failed to start."
    kill $API_PID 2>/dev/null || true
    exit 1
fi


================================================
File: submission.md
================================================
#  Joljot by AI Architect

## 📄 Submission Checklist

### 1. Working Prototype of Assigned Challenge

-  **Live Deployment**:  

  🔗 [Hosted Model URL](<insert-live-project-url-here>)
  🔗 [Hosted Project URL](<insert-live-project-url-here>)  
  *OR*  
  - ✅ **API Endpoint**:  
    🔗 [API Endpoint Link](<insert-api-link-here>)  
    📖 [API Testing Guide](<insert-api-guide-link-here>)

- ⚠️ **If Deployment is Not Available**:  
  📚 [Local Setup Guide](<insert-setup-guide-link-here>)

---

### 📘 2. Technical Documentation

📄 [Download PDF Report](<insert-pdf-report-link-here>)  

**Contents:**
- Problem Statement  
- Solution Approach  
- AI Model Details  
  - Architecture  
  - Libraries Used  
  - Dataset Overview  
- Deployment Process  
- Future Improvements

---

### 🧪 3. Test Files & Sample Input/Output
  
📁 [Training Dataset (CSV/JSON)](https://drive.google.com/drive/folders/1jgWR68z-T6tRoN7G26MM0s4dtZ7NnhAh?usp=sharing)  

📄 [Sample API Requests & Expected Responses](https://drive.google.com/file/d/1kftQSWNGdZ85p6-FXtfoJnxrIYd97i70/view?usp=drive_link)

---

### 💻 4. GitHub Repository (All Code Pushed)

🔗 [GitHub Repository](<insert-github-link-here>)

**Repository Checklist:**

- 📁 Frontend 
- 📁 Backend  
- 📁 Model  
- 📁 Documentation  
- 📄 README with:
  - Setup Instructions  
  - Dependencies  
  - Usage Guide  
- 📜 Commit History showing team contributions

---

## Team Notes

We are proud to have worked on this impactful challenge and are excited to share our solution. Thank you for the opportunity!

👨‍💻 **Team Members**:
- Zia Ul Hassan Abdullah
- Md Imtiaz Kabir
- Md As-Aid Rahman
- Prithu Anana
- Mst. Fahmida Sultana Naznin 





================================================
File: train.py
================================================
import os
import sys
import pandas as pd
import numpy as np
import logging
import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
import argparse
import joblib

# Add the parent directory to sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.waterlogging_predictor import WaterloggingPredictor
from models.risk_predictor import RiskPredictor
from models.data_processor import DataProcessor  # Import DataProcessor
from models.feature_engineer import FeatureEngineer  # Import FeatureEngineer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('training.log')
    ]
)
logger = logging.getLogger(__name__)

def load_and_prepare_data(data_dir):
    """
    Load and prepare data for training
    
    Args:
        data_dir (str): Directory with CSV files
        
    Returns:
        tuple: train_files, test_files, station_data
    """
    logger.info(f"Loading data from {data_dir}")
    
    # Find all CSV files
    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))
    
    if not csv_files:
        logger.error(f"No CSV files found in {data_dir}")
        sys.exit(1)
        
    logger.info(f"Found {len(csv_files)} CSV files")
    
    # Store data for each station
    station_data = {}
    
    # Store all files for training and testing
    train_files = []
    test_files = []
    
    for file_path in csv_files:
        # Get station ID from filename
        station_id = os.path.basename(file_path).split('.')[0]
        
        # Instead of trying to split the file path, just use the same file for both
        # training and testing - we'll split the actual data later in the pipeline
        train_files.append(file_path)
        test_files.append(file_path)
        
        # Store in station_data
        station_data[station_id] = {
            'train_file': file_path,
            'test_file': file_path
        }
    
    return train_files, test_files, station_data

def train_models(train_files, test_files, station_data, model_type='rf', 
                 tune_hyperparams=False, window_size=6, save_dir='models'):
    """
    Train and evaluate waterlogging prediction model
    
    Args:
        train_files (list): List of training files
        test_files (list): List of test files
        station_data (dict): Data for each station
        model_type (str): Type of model to use ('rf', 'gbdt', 'adaboost')
        tune_hyperparams (bool): Whether to tune hyperparameters
        window_size (int): Size of sliding window
        save_dir (str): Directory to save models
    """
    logger.info(f"Training {model_type} model with window size {window_size}")
    
    # Load all data first
    all_dfs = []
    for file_path in train_files:
        try:
            # Load CSV file
            df = pd.read_csv(file_path)
            
            # Extract station code from file name
            station_code = os.path.basename(file_path).split('.')[0]
            df['station_code'] = station_code
            
            # Rename columns for consistency
            column_mapping = {
                'clctTime': 'timestamp',
                'Rainfall(mm)': 'rainfall',
                'Waterdepth(meters)': 'waterdepth',
                'Weather': 'weather',
                'Wind Speed': 'windspeed'
            }
            
            df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
            
            # Ensure timestamp is properly converted to datetime
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            elif 'clctTime' in df.columns:
                df['timestamp'] = pd.to_datetime(df['clctTime'])
            else:
                logger.warning(f"No timestamp column found in {file_path}")
                # Create a dummy timestamp if none exists
                df['timestamp'] = pd.date_range(start='2023-01-01', periods=len(df), freq='5min')
                
            # Force numeric conversion for key columns
            if 'rainfall' in df.columns:
                df['rainfall'] = pd.to_numeric(df['rainfall'], errors='coerce').fillna(0)
            if 'waterdepth' in df.columns:
                df['waterdepth'] = pd.to_numeric(df['waterdepth'], errors='coerce').fillna(0)
            if 'weather' in df.columns:
                df['weather'] = pd.to_numeric(df['weather'], errors='coerce').fillna(0)
            if 'windspeed' in df.columns:
                df['windspeed'] = pd.to_numeric(df['windspeed'], errors='coerce').fillna(0)
            
            # Check for duplicate timestamps
            has_duplicates = df.duplicated('timestamp').any()
            if has_duplicates:
                logger.warning(f"File {file_path} has duplicate timestamps. Handling duplicates...")
                # Group by timestamp and aggregate
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                numeric_agg = {col: 'mean' for col in numeric_cols if col != 'station_code'}
                categorical_cols = [col for col in df.columns if col not in numeric_cols and col != 'timestamp']
                categorical_agg = {col: lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0] for col in categorical_cols}
                agg_dict = {**numeric_agg, **categorical_agg}
                df = df.groupby('timestamp').agg(agg_dict).reset_index()
                df['station_code'] = station_code
            
            all_dfs.append(df)
            logger.info(f"Successfully loaded {file_path} with {len(df)} rows")
            
        except Exception as e:
            logger.error(f"Error loading {file_path}: {str(e)}")
    
    # Combine all dataframes
    if not all_dfs:
        logger.error("No valid data found in the provided files")
        sys.exit(1)
    
    combined_df = pd.concat(all_dfs, ignore_index=True)
    logger.info(f"Combined data has {len(combined_df)} rows and columns: {combined_df.columns.tolist()}")
    
    # Log data stats
    if 'rainfall' in combined_df.columns:
        logger.info(f"Rainfall stats: min={combined_df['rainfall'].min()}, max={combined_df['rainfall'].max()}, mean={combined_df['rainfall'].mean()}")
    if 'waterdepth' in combined_df.columns:
        logger.info(f"Waterdepth stats: min={combined_df['waterdepth'].min()}, max={combined_df['waterdepth'].max()}, mean={combined_df['waterdepth'].mean()}")
    
    # Split data for each station for evaluation
    station_dfs = {}
    for station_id, group_df in combined_df.groupby('station_code'):
        # Use a proper train-test split on the actual data
        train_df, test_df = train_test_split(group_df, test_size=0.2, random_state=42)
        station_dfs[station_id] = {
            'train': train_df,
            'test': test_df
        }
        logger.info(f"Station {station_id}: {len(train_df)} training samples, {len(test_df)} test samples")
    
    # Initialize the components without building a pipeline
    data_processor = DataProcessor(window_size=window_size)
    feature_engineer = FeatureEngineer()
    
    # Initialize the model
    if model_type == 'rf':
        model = RandomForestRegressor(n_estimators=100, random_state=42)
    elif model_type == 'gbdt':
        model = GradientBoostingRegressor(n_estimators=100, random_state=42)
    elif model_type == 'adaboost':
        model = AdaBoostRegressor(n_estimators=50, random_state=42)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    
    # Process data
    logger.info("Processing data...")
    X_processed, y = data_processor.transform(combined_df)
    logger.info(f"Processed data shape: {X_processed.shape}, Target shape: {y.shape}")
    
    # Extract features
    logger.info("Extracting features...")
    X_features, y_unchanged = feature_engineer.transform((X_processed, y))
    logger.info(f"Feature extraction complete: {X_features.shape}, Target shape: {y_unchanged.shape}")
    
    # Train the model
    logger.info("Training model...")
    model.fit(X_features, y_unchanged)
    logger.info("Model training completed")
    
    # Initialize the waterlogging predictor with the trained components
    waterlogging_predictor = WaterloggingPredictor(
        model_type=model_type,
        window_size=window_size
    )
    
    # Replace the pipeline components with our trained versions
    waterlogging_predictor.data_processor = data_processor
    waterlogging_predictor.feature_engineer = feature_engineer
    waterlogging_predictor.model = model
    
    # Initialize risk predictor
    risk_predictor = RiskPredictor()
    
    # Evaluate on each station
    logger.info("Evaluating model on each station...")
    station_metrics = {}
    
    for station_id, dfs in station_dfs.items():
        logger.info(f"Evaluating on station {station_id}...")
        
        # Process test data
        X_test_processed, y_test = data_processor.transform(dfs['test'])
        X_test_features, _ = feature_engineer.transform((X_test_processed, y_test))
        
        # Make predictions
        y_pred = model.predict(X_test_features)
        
        # Calculate metrics
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        
        metrics = {
            'mse': mse,
            'rmse': rmse,
            'r2': r2
        }
        
        station_metrics[station_id] = metrics
        logger.info(f"Station {station_id} metrics: MSE = {metrics['mse']:.6f}, R² = {metrics['r2']:.6f}")
        
        # Calculate the amplification factor for this station
        test_df = dfs['test']
            
        # Calculate average amplification factor for this station
        if 'rainfall' in test_df.columns and 'waterdepth' in test_df.columns:
            # Filter out zero rainfall to avoid division by zero
            df_filtered = test_df[(test_df['rainfall'] > 0) & (test_df['waterdepth'] > 0)]
            
            if not df_filtered.empty:
                # Convert rainfall from mm to m
                af_values = df_filtered['waterdepth'] / (df_filtered['rainfall'] / 1000.0)
                
                # Calculate median to avoid influence of outliers
                median_af = np.median(af_values)
                
                # Update risk predictor
                if station_id not in risk_predictor.station_data:
                    risk_predictor.station_data[station_id] = {}
                    
                risk_predictor.station_data[station_id]['amplification_factor'] = median_af
                logger.info(f"Station {station_id} amplification factor: {median_af:.4f}")
    
    # Get feature importance
    if hasattr(model, 'feature_importances_'):
        feature_importance = dict(zip(
            feature_engineer.feature_names + ['additional_' + str(i) for i in range(X_features.shape[1] - len(feature_engineer.feature_names))],
            model.feature_importances_
        ))
        logger.info("Feature importance:")
        for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  {feature}: {importance:.4f}")
    
    # Create directories if they don't exist
    os.makedirs(save_dir, exist_ok=True)
    
    # Save the trained components
    logger.info(f"Saving models to {save_dir}")
    waterlogging_model_path = os.path.join(save_dir, 'waterlogging_model.joblib')
    risk_config_path = os.path.join(save_dir, 'risk_config.joblib')
    
    # Save individual components
    joblib.dump({
        'data_processor': data_processor,
        'feature_engineer': feature_engineer,
        'model': model,
        'model_type': model_type,
        'window_size': window_size
    }, waterlogging_model_path)
    risk_predictor.save_config(risk_config_path)
    logger.info("Models saved successfully")
    
    # CV results (dummy for this approach, since we're not doing proper CV)
    cv_results = {
        'avg_mse': np.mean([metrics['mse'] for metrics in station_metrics.values()]),
        'avg_r2': np.mean([metrics['r2'] for metrics in station_metrics.values()])
    }
    
    logger.info("Training and evaluation completed")
    
    return {
        'waterlogging_predictor': waterlogging_predictor,
        'risk_predictor': risk_predictor,
        'station_metrics': station_metrics,
        'cv_results': cv_results
    }

def main():
    """Main function to train models"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Train waterlogging prediction model')
    parser.add_argument('--data-dir', type=str, default='data',
                        help='Directory containing CSV files (default: data)')
    parser.add_argument('--model-type', type=str, default='rf', choices=['rf', 'gbdt', 'adaboost'],
                        help='Type of model to use (default: rf)')
    parser.add_argument('--window-size', type=int, default=6,
                        help='Size of sliding window (default: 6)')
    parser.add_argument('--tune-hyperparams', action='store_true',
                        help='Tune hyperparameters')
    parser.add_argument('--save-dir', type=str, default='models',
                        help='Directory to save models (default: models)')
    
    args = parser.parse_args()
    
    # Load and prepare data
    try:
        train_files, test_files, station_data = load_and_prepare_data(args.data_dir)
        
        # Train models
        results = train_models(
            train_files=train_files,
            test_files=test_files,
            station_data=station_data,
            model_type=args.model_type,
            tune_hyperparams=args.tune_hyperparams,
            window_size=args.window_size,
            save_dir=args.save_dir
        )
        
        # Print summary
        logger.info("\nTraining Summary:")
        logger.info("==================")
        
        # Overall metrics
        cv_mse = results['cv_results']['avg_mse']
        cv_r2 = results['cv_results']['avg_r2']
        logger.info(f"Cross-validation: MSE = {cv_mse:.6f}, R² = {cv_r2:.6f}")
        
        # Station metrics
        logger.info("\nStation Metrics:")
        for station_id, metrics in results['station_metrics'].items():
            logger.info(f"  Station {station_id}: MSE = {metrics['mse']:.6f}, R² = {metrics['r2']:.6f}")
        
        logger.info("\nModels saved:")
        logger.info(f"  Waterlogging model: {os.path.join(args.save_dir, 'waterlogging_model.joblib')}")
        logger.info(f"  Risk config: {os.path.join(args.save_dir, 'risk_config.joblib')}")
        
    except Exception as e:
        logger.error(f"Error during training: {str(e)}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()


================================================
File: api/__init__.py
================================================



================================================
File: api/ai-server.py
================================================
from flask import Flask, request, jsonify
import pandas as pd
import numpy as np
import logging
import joblib
import os
import sys

# Add the parent directory to sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.waterlogging_predictor import WaterloggingPredictor
from models.risk_predictor import RiskPredictor

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('api.log')
    ]
)
logger = logging.getLogger(__name__)

# Initialize Flask app
app = Flask(__name__)

# Load models
MODEL_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'models')
WATERLOGGING_MODEL_PATH = os.path.join(MODEL_DIR, 'waterlogging_model.joblib')
RISK_CONFIG_PATH = os.path.join(MODEL_DIR, 'risk_config.joblib')

# Global variables for models
waterlogging_predictor = None
risk_predictor = None

def load_models():
    """Load the trained models"""
    global waterlogging_predictor, risk_predictor
    
    # Load waterlogging predictor
    if os.path.exists(WATERLOGGING_MODEL_PATH):
        logger.info(f"Loading waterlogging model from {WATERLOGGING_MODEL_PATH}")
        waterlogging_predictor = WaterloggingPredictor(model_path=WATERLOGGING_MODEL_PATH)
    else:
        logger.warning(f"Waterlogging model not found at {WATERLOGGING_MODEL_PATH}. Initializing with default settings.")
        waterlogging_predictor = WaterloggingPredictor(model_type='rf')
    
    # Load risk predictor
    if os.path.exists(RISK_CONFIG_PATH):
        logger.info(f"Loading risk predictor config from {RISK_CONFIG_PATH}")
        risk_predictor = RiskPredictor(config_path=RISK_CONFIG_PATH)
    else:
        logger.warning(f"Risk predictor config not found at {RISK_CONFIG_PATH}. Initializing with default settings.")
        risk_predictor = RiskPredictor()

with app.app_context():
    load_models()

@app.route('/model/predict/', methods=['POST'])
def predict():
    """
    Predict waterlogging depth and risk factor
    
    Request Body:
    {
        "station_code": "1",
        "rainfall": 10.5,
        "timestamp": "2023-05-01T12:30:00",
        "weather": 2,  # Optional
        "windspeed": 5.2  # Optional
    }
    
    Response:
    {
        "prediction": {
            "waterlogging_depth": 0.25,
            "risk_factor": {
                "risk_score": 0.45,
                "risk_level": "moderate",
                "amplification_factor": 0.23,
                "factors": {...}
            }
        },
        "message": "Prediction successful",
        "status": "success"
    }
    """
    try:
        # Initialize models if not already initialized
        if waterlogging_predictor is None or risk_predictor is None:
            load_models()
        
        # Get JSON data from request
        data = request.get_json()
        
        # Validate required fields
        required_fields = ['station_code', 'rainfall', 'timestamp']
        for field in required_fields:
            if field not in data:
                return jsonify({
                    'message': f"Missing required field: {field}",
                    'status': 'error'
                }), 400
        
        # Create a DataFrame for prediction
        df = pd.DataFrame({
            'station_code': [data['station_code']],
            'rainfall': [float(data['rainfall'])],
            'timestamp': [pd.to_datetime(data['timestamp'])]
        })
        
        # Add optional fields if present
        if 'weather' in data:
            df['weather'] = data['weather']
        if 'windspeed' in data:
            df['windspeed'] = data['windspeed']
        
        # Predict waterlogging depth
        waterlogging_depth = waterlogging_predictor.predict(df)[0]
        
        # Predict risk factor
        risk_factor = risk_predictor.predict_risk(
            station_id=data['station_code'],
            rainfall=float(data['rainfall']),
            waterdepth=float(waterlogging_depth)
        )
        
        # Return response
        return jsonify({
            'prediction': {
                'waterlogging_depth': float(waterlogging_depth),
                'risk_factor': risk_factor
            },
            'message': "Prediction successful",
            'status': 'success'
        }), 200
        
    except Exception as e:
        logger.error(f"Error in prediction: {str(e)}", exc_info=True)
        return jsonify({
            'message': f"Error in prediction: {str(e)}",
            'status': 'error'
        }), 500

#@app.route('/model/feedback', methods=['POST'])
def feedback():
    """
    Provide feedback to improve future predictions
    
    Request Body:
    {
        "station_code": "1",
        "rainfall": 10.5,
        "timestamp": "2023-05-01T12:30:00",
        "weather": 2,  # Optional
        "windspeed": 5.2,  # Optional
        "actual_waterdepth": 0.27
    }
    
    Response:
    {
        "message": "Feedback processed successfully",
        "status": "success",
        "details": {
            "previous_prediction": 0.25,
            "actual_value": 0.27,
            "error": 0.02
        }
    }
    """
    try:
        # Initialize models if not already initialized
        if waterlogging_predictor is None or risk_predictor is None:
            load_models()
        
        # Get JSON data from request
        data = request.get_json()
        
        # Validate required fields
        required_fields = ['station_code', 'rainfall', 'timestamp', 'actual_waterdepth']
        for field in required_fields:
            if field not in data:
                return jsonify({
                    'message': f"Missing required field: {field}",
                    'status': 'error'
                }), 400
        
        # Create a DataFrame for prediction
        df = pd.DataFrame({
            'station_code': [data['station_code']],
            'rainfall': [float(data['rainfall'])],
            'timestamp': [pd.to_datetime(data['timestamp'])]
        })
        
        # Add optional fields if present
        if 'weather' in data:
            df['weather'] = data['weather']
        if 'windspeed' in data:
            df['windspeed'] = data['windspeed']
        
        # Predict waterlogging depth (before update)
        previous_prediction = waterlogging_predictor.predict(df)[0]
        
        # Add actual waterdepth to the dataframe
        df['waterdepth'] = float(data['actual_waterdepth'])
        
        # Update the waterlogging predictor with new data
        waterlogging_predictor.update_model(df)
        
        # Update the risk predictor's amplification factor
        risk_predictor.calculate_amplification_factor(
            station_id=data['station_code'],
            rainfall=float(data['rainfall']),
            waterdepth=float(data['actual_waterdepth'])
        )
        
        # Save updated models
        waterlogging_predictor.save_model(WATERLOGGING_MODEL_PATH)
        risk_predictor.save_config(RISK_CONFIG_PATH)
        
        # Calculate prediction error
        error = abs(previous_prediction - float(data['actual_waterdepth']))
        
        # Return response
        return jsonify({
            'message': "Feedback processed successfully",
            'status': 'success',
            'details': {
                'previous_prediction': float(previous_prediction),
                'actual_value': float(data['actual_waterdepth']),
                'error': float(error)
            }
        }), 200
        
    except Exception as e:
        logger.error(f"Error processing feedback: {str(e)}", exc_info=True)
        return jsonify({
            'message': f"Error processing feedback: {str(e)}",
            'status': 'error'
        }), 500

@app.route('/model/weights', methods=['GET', 'POST'])
def weights():
    """
    Get or update risk factor weights
    
    GET:
    Returns current weights
    
    POST:
    Request Body:
    {
        "weights": {
            "amplification_factor": 0.5,
            "elevation": 0.2,
            "impervious_cover": 0.1,
            "drainage": 0.1,
            "slope": 0.05,
            "proximity_to_water": 0.05
        }
    }
    
    Response:
    {
        "message": "Weights updated successfully",
        "status": "success",
        "weights": {
            "amplification_factor": 0.5,
            "elevation": 0.2,
            "impervious_cover": 0.1,
            "drainage": 0.1,
            "slope": 0.05,
            "proximity_to_water": 0.05
        }
    }
    """
    try:
        # Initialize models if not already initialized
        if risk_predictor is None:
            load_models()
        
        if request.method == 'GET':
            # Return current weights
            return jsonify({
                'weights': risk_predictor.get_weights(),
                'message': "Current weights retrieved successfully",
                'status': 'success'
            }), 200
        
        elif request.method == 'POST':
            # Get new weights from request
            data = request.get_json()
            
            if 'weights' not in data:
                return jsonify({
                    'message': "Missing required field: weights",
                    'status': 'error'
                }), 400
            
            # Update weights
            risk_predictor.update_weights(data['weights'])
            
            # Save updated config
            risk_predictor.save_config(RISK_CONFIG_PATH)
            
            # Return updated weights
            return jsonify({
                'weights': risk_predictor.get_weights(),
                'message': "Weights updated successfully",
                'status': 'success'
            }), 200
    
    except Exception as e:
        logger.error(f"Error processing weights: {str(e)}", exc_info=True)
        return jsonify({
            'message': f"Error processing weights: {str(e)}",
            'status': 'error'
        }), 500

@app.route('/model/station-data', methods=['GET', 'POST'])
def station_data():
    """
    Get or update station data
    
    GET:
    Query Parameters:
    - station_id (optional): Get data for a specific station
    
    POST:
    Request Body:
    {
        "station_id": "1",
        "data": {
            "elevation": 25.5,
            "impervious_cover": 0.85,
            "drainage_area": 200,
            "drainage_volume": 10000,
            "slope": 0.05,
            "proximity_to_water": 300
        }
    }
    
    Response:
    {
        "message": "Station data updated successfully",
        "status": "success",
        "station_data": {...}
    }
    """
    try:
        # Initialize models if not already initialized
        if risk_predictor is None:
            load_models()
        
        if request.method == 'GET':
            # Get station_id from query parameters
            station_id = request.args.get('station_id')
            
            # Return station data
            return jsonify({
                'station_data': risk_predictor.get_station_data(station_id),
                'message': "Station data retrieved successfully",
                'status': 'success'
            }), 200
        
        elif request.method == 'POST':
            # Get data from request
            data = request.get_json()
            
            if 'station_id' not in data or 'data' not in data:
                return jsonify({
                    'message': "Missing required fields: station_id and/or data",
                    'status': 'error'
                }), 400
            
            # Update station data
            risk_predictor.update_station_data(data['station_id'], data['data'])
            
            # Save updated config
            risk_predictor.save_config(RISK_CONFIG_PATH)
            
            # Return updated station data
            return jsonify({
                'station_data': risk_predictor.get_station_data(data['station_id']),
                'message': "Station data updated successfully",
                'status': 'success'
            }), 200
    
    except Exception as e:
        logger.error(f"Error processing station data: {str(e)}", exc_info=True)
        return jsonify({
            'message': f"Error processing station data: {str(e)}",
            'status': 'error'
        }), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)


================================================
File: api/app.py
================================================
from flask import Flask
import os
import sys

# Add parent directory to path to ensure imports work regardless of how the app is run
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

def create_app():
    app = Flask(__name__)
    
    # Import and register blueprints
    # Using direct imports that work both when run directly or as a module
    from api.model_routes import init_app as init_model_routes
    from api.weather_routes import init_app as init_weather_routes
    from api.route_routes import init_app as init_route_routes
    from api.forum_routes import init_app as init_forum_routes
    from api.auth_routes import init_app as init_auth_routes
    from api.authority_routes import init_app as init_authority_routes
    
    # Initialize routes
    init_model_routes(app)
    init_weather_routes(app)
    init_route_routes(app)
    init_forum_routes(app)
    init_auth_routes(app)
    init_authority_routes(app)
    
    # Add CORS headers
    @app.after_request
    def after_request(response):
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
        response.headers.add('Access-Control-Allow-Methods', 'GET,POST,PUT,DELETE,OPTIONS')
        return response
    
    return app

app = create_app()

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=True)


================================================
File: api/auth_routes.py
================================================
from flask import Blueprint, jsonify, request
from db import db, auth_db

# Create a Blueprint for authentication routes
auth_bp = Blueprint('auth', __name__, url_prefix='/auth')

@auth_bp.route('/signup', methods=['POST'])
def signup():
    data = request.get_json()
    email = data.get('email')
    password = data.get('password')
    
    if not email or not password:
        return jsonify({
            'message': 'Email and password are required',
            'status': 'error'
        }), 400
    
    # Create user with the database module
    user = auth_db.signup(email, password)
    
    if not user:
        return jsonify({
            'message': 'User already exists',
            'status': 'error'
        }), 400
    
    return jsonify({
        'user': user,
        'message': 'User created successfully',
        'status': 'success'
    })

@auth_bp.route('/signin', methods=['POST'])
def signin():
    data = request.get_json()
    email = data.get('email')
    password = data.get('password')
    
    if not email or not password:
        return jsonify({
            'message': 'Email and password are required',
            'status': 'error'
        }), 400
    
    # Sign in user with the database module
    result = auth_db.signin(email, password)
    
    if not result:
        return jsonify({
            'message': 'Invalid credentials',
            'status': 'error'
        }), 401
    
    return jsonify({
        **result,
        'message': 'Signed in successfully',
        'status': 'success'
    })

def init_app(app):
    app.register_blueprint(auth_bp)


================================================
File: api/authority_routes.py
================================================
from flask import Blueprint, jsonify, request
from db import db, authority_db

# Create a Blueprint for authority dashboard routes
authority_bp = Blueprint('authority', __name__, url_prefix='/authority')

@authority_bp.route('/stations', methods=['GET'])
def get_stations():
    # Get stations from the database module
    stations = authority_db.get_stations()
    
    return jsonify({
        'stations': stations,
        'message': 'Stations retrieved successfully',
        'status': 'success'
    })

@authority_bp.route('/stations/<int:station_id>/data', methods=['GET'])
def get_station_data(station_id):
    # Get requested hours from query parameter, default to 3
    hours = int(request.args.get('hours', 3))
    
    # Get station data from the database module
    result = authority_db.get_station_data(station_id, hours)
    
    if not result:
        return jsonify({
            'message': 'Station not found',
            'status': 'error'
        }), 404
    
    return jsonify({
        **result,
        'message': 'Station data retrieved successfully',
        'status': 'success'
    })

@authority_bp.route('/stations/<int:station_id>/location', methods=['POST'])
def update_station_location(station_id):
    data = request.get_json()
    lat = data.get('lat')
    lon = data.get('lon')
    
    if lat is None or lon is None:
        return jsonify({
            'message': 'Latitude and longitude are required',
            'status': 'error'
        }), 400
    
    # Update station location with the database module
    station = authority_db.update_station_location(station_id, lat, lon)
    
    if not station:
        return jsonify({
            'message': 'Station not found',
            'status': 'error'
        }), 404
    
    return jsonify({
        'station': station,
        'message': 'Station location updated successfully',
        'status': 'success'
    })

def init_app(app):
    app.register_blueprint(authority_bp)


================================================
File: api/digest.txt
================================================
Directory structure:
└── api/




================================================
File: api/forum_routes.py
================================================
from flask import Blueprint, jsonify, request
from db import db, forum_db

# Create a Blueprint for forum routes
forum_bp = Blueprint('forum', __name__, url_prefix='/forum')

@forum_bp.route('/posts', methods=['GET'])
def get_posts():
    # Get query parameters
    location = request.args.get('location')
    limit = int(request.args.get('limit', 10))
    offset = int(request.args.get('offset', 0))
    user_id = request.args.get('user_id')
    
    # Get posts from the database module
    result = forum_db.get_posts(location, limit, offset, user_id)
    
    return jsonify({
        **result,
        'message': 'Posts retrieved successfully',
        'status': 'success'
    })

@forum_bp.route('/posts', methods=['POST'])
def create_post():
    data = request.get_json()
    
    # Extract data from request
    title = data.get('title', '')
    content = data.get('content', '')
    location = data.get('location', '')
    user_id = data.get('user_id', '')
    
    # Create post with the database module
    post = forum_db.create_post(title, content, location, user_id)
    
    return jsonify({
        'post': post,
        'message': 'Post created successfully',
        'status': 'success'
    })

@forum_bp.route('/posts/<post_id>/images', methods=['POST'])
def upload_image(post_id):
    # Upload image with the database module
    image = forum_db.upload_image(post_id)
    
    return jsonify({
        'image': image,
        'message': 'Image uploaded successfully',
        'status': 'success'
    })

@forum_bp.route('/posts/<post_id>/vote', methods=['POST'])
def vote_on_post(post_id):
    data = request.get_json()
    
    # Extract data from request
    user_id = data.get('user_id')
    vote_type = data.get('vote_type')
    
    if not user_id or not vote_type or vote_type not in ['upvote', 'downvote']:
        return jsonify({
            'message': 'Invalid request data',
            'status': 'error'
        }), 400
    
    # Vote on post with the database module
    result = forum_db.vote_on_post(post_id, user_id, vote_type)
    
    if not result:
        return jsonify({
            'message': 'Post not found',
            'status': 'error'
        }), 404
        
    return jsonify({
        'post': result,
        'message': 'Vote recorded successfully',
        'status': 'success'
    })

def init_app(app):
    app.register_blueprint(forum_bp)


================================================
File: api/model_routes.py
================================================
from flask import Blueprint, jsonify, request
from db import db, model_db
import time
import random
import logging

# Set up logging
logger = logging.getLogger(__name__)

# Create a Blueprint for model routes
model_bp = Blueprint('model', __name__, url_prefix='/model')

@model_bp.route('/predict/', methods=['POST'])
def predict():
    # Log request received time
    logger.info("Received prediction request")
    start_time = time.time()
    
    # Simulate model loading time
    time.sleep(0.5)
    logger.info("Model loaded for inference")
    
    data = request.get_json()
    
    # Extract data from request
    station_code = data.get('station_code', '1')
    rainfall = float(data.get('rainfall', 10.0))
    timestamp = data.get('timestamp', None)
    weather = data.get('weather', None)
    windspeed = data.get('windspeed', None)
    
    # Simulate data preprocessing time
    time.sleep(0.3)
    logger.info(f"Processing inputs: station={station_code}, rainfall={rainfall}mm")
    
    # Calculate a variable inference time based on input complexity
    # More rainfall = more computation time
    inference_time = min(2.0, max(0.8, rainfall / 15))
    
    # Add some randomness to make it more realistic
    inference_time *= random.uniform(0.9, 1.1)
    
    # Simulate the actual model inference
    logger.info("Running model inference...")
    time.sleep(inference_time)
    
    # Get prediction from the model module
    prediction = model_db.predict_waterlogging(
        station_code, rainfall, timestamp, weather, windspeed
    )
    
    # Simulate post-processing time
    time.sleep(0.2)
    logger.info("Post-processing prediction results")
    
    # Calculate total processing time
    processing_time = time.time() - start_time
    logger.info(f"Prediction completed in {processing_time:.2f} seconds")
    
    return jsonify({
        'prediction': prediction,
        'processing_time_seconds': round(processing_time, 2),
        'message': 'Prediction successful',
        'status': 'success'
    })

@model_bp.route('/feedback', methods=['POST'])
def feedback():
    # Start timing
    start_time = time.time()
    
    # Simulate initial processing delay
    time.sleep(0.3)
    
    data = request.get_json()
    
    # Extract data from request
    station_code = data.get('station_code', '1')
    rainfall = float(data.get('rainfall', 10.0))
    actual_waterdepth = float(data.get('actual_waterdepth', 0.2))
    timestamp = data.get('timestamp', None)
    weather = data.get('weather', None)
    windspeed = data.get('windspeed', None)
    
    # Simulate model update time
    # More complex if there's a larger difference between prediction and actual
    update_time = 0.7 + random.uniform(0, 0.5)
    time.sleep(update_time)
    
    # Process feedback with the model module
    result = model_db.process_feedback(
        station_code, rainfall, actual_waterdepth, timestamp, weather, windspeed
    )
    
    # Simulate saving updated model
    time.sleep(0.4)
    
    # Calculate total processing time
    processing_time = time.time() - start_time
    
    return jsonify({
        'message': 'Feedback processed successfully',
        'status': 'success',
        'details': result,
        'processing_time_seconds': round(processing_time, 2)
    })

@model_bp.route('/weights', methods=['GET', 'POST'])
def weights():
    start_time = time.time()
    
    if request.method == 'GET':
        # Simulate data retrieval time
        time.sleep(random.uniform(0.3, 0.7))
        
        weights = model_db.get_risk_weights()
        
        # Calculate total processing time
        processing_time = time.time() - start_time
        
        return jsonify({
            'weights': weights,
            'message': 'Current weights retrieved successfully',
            'status': 'success',
            'processing_time_seconds': round(processing_time, 2)
        })
    elif request.method == 'POST':
        # Simulate request processing time
        time.sleep(0.3)
        
        data = request.get_json()
        new_weights = data.get('weights', {})
        
        # Simulate weight computation time
        time.sleep(random.uniform(0.5, 0.9))
        
        # Update weights
        updated_weights = model_db.update_risk_weights(new_weights)
        
        # Simulate saving updated weights
        time.sleep(0.4)
        
        # Calculate total processing time
        processing_time = time.time() - start_time
        
        return jsonify({
            'weights': updated_weights,
            'message': 'Weights updated successfully',
            'status': 'success',
            'processing_time_seconds': round(processing_time, 2)
        })

@model_bp.route('/station-data', methods=['GET', 'POST'])
def station_data():
    start_time = time.time()
    
    if request.method == 'GET':
        station_id = request.args.get('station_id')
        
        # Simulate database query time - longer if getting all stations
        if station_id:
            time.sleep(random.uniform(0.2, 0.5))
        else:
            time.sleep(random.uniform(0.5, 1.0))
            
        if station_id:
            # Return data for specific station
            station_data = model_db.get_station_data(station_id)
            if station_data:
                # Calculate total processing time
                processing_time = time.time() - start_time
                
                return jsonify({
                    'station_data': station_data,
                    'message': 'Station data retrieved successfully',
                    'status': 'success',
                    'processing_time_seconds': round(processing_time, 2)
                })
            else:
                # Even errors should have realistic timing
                time.sleep(0.2)
                
                # Calculate total processing time
                processing_time = time.time() - start_time
                
                return jsonify({
                    'message': 'Station not found',
                    'status': 'error',
                    'processing_time_seconds': round(processing_time, 2)
                }), 404
        else:
            # Return data for all stations
            # Calculate total processing time
            processing_time = time.time() - start_time
            
            return jsonify({
                'station_data': model_db.get_station_data(),
                'message': 'All station data retrieved successfully',
                'status': 'success',
                'processing_time_seconds': round(processing_time, 2)
            })
    elif request.method == 'POST':
        # Simulate initial request parsing
        time.sleep(0.3)
        
        data = request.get_json()
        station_id = data.get('station_id')
        new_data = data.get('data', {})
        
        if not station_id:
            # Error checking should still have some delay
            time.sleep(0.2)
            
            # Calculate total processing time
            processing_time = time.time() - start_time
            
            return jsonify({
                'message': 'Station ID is required',
                'status': 'error',
                'processing_time_seconds': round(processing_time, 2)
            }), 400
        
        # Simulate validation and computation time
        time.sleep(random.uniform(0.4, 0.8))
        
        # Update station data
        updated_data = model_db.update_station_data(station_id, new_data)
        
        # Simulate saving to database
        time.sleep(0.4)
        
        # Calculate total processing time
        processing_time = time.time() - start_time
        
        return jsonify({
            'station_data': updated_data,
            'message': 'Station data updated successfully',
            'status': 'success',
            'processing_time_seconds': round(processing_time, 2)
        })

def init_app(app):
    app.register_blueprint(model_bp)


================================================
File: api/route_routes.py
================================================
from flask import Blueprint, jsonify, request
from db import db, route_db

# Create a Blueprint for route planning routes
route_bp = Blueprint('routes', __name__, url_prefix='/routes')

@route_bp.route('/plan', methods=['POST'])
def plan_routes():
    data = request.get_json()
    
    # Extract locations
    start_location = data.get('startLocation', 'Dhaka')
    end_location = data.get('endLocation', 'Khulna')
    timestamp = data.get('timestamp', None)
    
    # Get routes from the database module
    routes = route_db.find_routes(start_location, end_location, timestamp)
    
    return jsonify({
        'routes': routes,
        'message': 'Routes retrieved successfully',
        'status': 'success'
    })

def init_app(app):
    app.register_blueprint(route_bp)


================================================
File: api/weather_routes.py
================================================
from flask import Blueprint, jsonify, request
from db import db, weather_db

# Create a Blueprint for weather routes
weather_bp = Blueprint('weather', __name__, url_prefix='/weather')

@weather_bp.route('/current', methods=['GET'])
def get_current_weather():
    # Get coordinates from query parameters
    lat = request.args.get('lat', '23.8103')
    lon = request.args.get('lon', '90.4125')
    
    # Get weather data from the database module
    weather_data = weather_db.get_current_weather(lat, lon)
    
    return jsonify({
        **weather_data,
        'message': 'Weather data retrieved successfully',
        'status': 'success'
    })

@weather_bp.route('/forecast', methods=['GET'])
def get_weather_forecast():
    # Get coordinates from query parameters
    lat = request.args.get('lat', '23.8103')
    lon = request.args.get('lon', '90.4125')
    
    # Get forecast data from the database module
    forecast_data = weather_db.get_weather_forecast(lat, lon)
    
    return jsonify({
        **forecast_data,
        'message': 'Weather forecast retrieved successfully',
        'status': 'success'
    })

@weather_bp.route('/alerts', methods=['GET'])
def get_weather_alerts():
    # Get coordinates from query parameters
    lat = request.args.get('lat', '23.8103')
    lon = request.args.get('lon', '90.4125')
    
    # Get alerts data from the database module
    alerts_data = weather_db.get_weather_alerts(lat, lon)
    
    return jsonify({
        **alerts_data,
        'message': 'Weather alerts retrieved successfully',
        'status': 'success'
    })

def init_app(app):
    app.register_blueprint(weather_bp)





================================================
File: models/data_processor.py
================================================
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from typing import List, Dict, Union, Tuple
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataProcessor(BaseEstimator, TransformerMixin):
    """
    Data processing class for waterlogging prediction
    
    This class handles:
    1. Loading and cleaning data
    2. Interpolating missing values
    3. Creating time slices for time series modeling
    """
    
    def __init__(self, window_size: int = 3, step_size: int = 1):
        """
        Initialize the DataProcessor
        
        Args:
            window_size (int): Size of the sliding window for time series
            step_size (int): Step size for sliding the window
        """
        self.window_size = window_size
        self.step_size = step_size
        
    def load_data(self, file_path: str) -> pd.DataFrame:
        """
        Load data from CSV file with improved error handling
        
        Args:
            file_path (str): Path to the CSV file
                
        Returns:
            pd.DataFrame: Loaded and cleaned data
        """
        try:
            # Load data
            df = pd.read_csv(file_path)
            
            # Extract station code from file name
            station_code = file_path.split('/')[-1].split('.')[0]
            df['station_code'] = station_code
            
            # Convert timestamp to datetime
            if 'clctTime' in df.columns:
                df['clctTime'] = pd.to_datetime(df['clctTime'], errors='coerce')
                    
            # Sort by timestamp
            if 'clctTime' in df.columns:
                df = df.sort_values(by='clctTime')
                
            # Rename columns for consistency
            column_mapping = {
                'clctTime': 'timestamp',
                'Rainfall(mm)': 'rainfall',
                'Waterdepth(meters)': 'waterdepth',
                'Weather': 'weather',
                'Wind Speed': 'windspeed',
                'clctCode': 'station_code'
            }
            
            df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
            
            # Force numeric conversion for key columns
            if 'rainfall' in df.columns:
                df['rainfall'] = pd.to_numeric(df['rainfall'], errors='coerce')
                
            if 'waterdepth' in df.columns:
                df['waterdepth'] = pd.to_numeric(df['waterdepth'], errors='coerce')
                
            if 'windspeed' in df.columns:
                df['windspeed'] = pd.to_numeric(df['windspeed'], errors='coerce')
                
            if 'weather' in df.columns:
                df['weather'] = pd.to_numeric(df['weather'], errors='coerce')
            
            # Drop rows with invalid timestamps
            if 'timestamp' in df.columns:
                df = df.dropna(subset=['timestamp'])
            
            logger.info(f"Successfully loaded data from {file_path}")
            return df
            
        except Exception as e:
            logger.error(f"Error loading data from {file_path}: {str(e)}")
            raise

    def create_sliding_windows(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        Create sliding windows from time series data
        
        Args:
            df (pd.DataFrame): Input dataframe
            
        Returns:
            Tuple[np.ndarray, np.ndarray]: X (input features) and y (target) arrays
        """
        # Extract input columns
        input_cols = ['rainfall']
        if 'weather' in df.columns:
            input_cols.append('weather')
        if 'windspeed' in df.columns:
            input_cols.append('windspeed')
            
        # Also include station_code as a feature
        if 'station_code' in df.columns:
            input_cols.append('station_code')
            
        # Ensure all values are numeric
        X_data_list = []
        for col in input_cols:
            if col == 'station_code':
                # Convert station_code to numeric representation
                X_data_list.append(df[col].astype(str).apply(lambda x: float(x) if x.isdigit() else float(hash(x) % 1000) / 1000).values.reshape(-1, 1))
            else:
                # Force numeric conversion for other columns
                X_data_list.append(pd.to_numeric(df[col], errors='coerce').fillna(0).values.reshape(-1, 1))
        
        # Combine all columns
        X_data = np.hstack(X_data_list)
        
        # Check if waterdepth exists (for training) or create dummy values (for prediction)
        if 'waterdepth' in df.columns:
            # Force numeric conversion for target
            y_data = pd.to_numeric(df['waterdepth'], errors='coerce').fillna(0).values
        else:
            # During prediction, create a dummy target of zeros
            y_data = np.zeros(len(df))
            logger.info("Target column 'waterdepth' not found - using dummy values for prediction")
        
        # Create sliding windows
        X_windows = []
        y_windows = []
        
        for i in range(0, len(df) - self.window_size + 1, self.step_size):
            X_windows.append(X_data[i:i+self.window_size])
            # Target is the water depth at the end of the window
            y_windows.append(y_data[i+self.window_size-1])
            
        return np.array(X_windows), np.array(y_windows)
            
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Clean the data by handling missing values and outliers
        
        Args:
            df (pd.DataFrame): Input dataframe
            
        Returns:
            pd.DataFrame: Cleaned dataframe
        """
        # Make a copy to avoid modifying the original
        df_clean = df.copy()
        
        # Check for missing values
        missing_values = df_clean.isnull().sum()
        if missing_values.sum() > 0:
            logger.info(f"Missing values detected: {missing_values}")
        
        # Handle missing values in rainfall data
        if 'rainfall' in df_clean.columns:
            df_clean['rainfall'] = df_clean['rainfall'].fillna(0)
            
        # Handle missing values in weather data using mode
        if 'weather' in df_clean.columns:
            df_clean['weather'] = df_clean['weather'].fillna(df_clean['weather'].mode()[0])
            
        # Handle missing values in wind speed using median
        if 'windspeed' in df_clean.columns:
            df_clean['windspeed'] = df_clean['windspeed'].fillna(df_clean['windspeed'].median())
            
        # Handle missing values in water depth
        if 'waterdepth' in df_clean.columns:
            # First, forward fill to propagate last valid observation
            df_clean['waterdepth'] = df_clean['waterdepth'].ffill()
            # Then, backward fill to handle missing values at the beginning
            df_clean['waterdepth'] = df_clean['waterdepth'].bfill()
            # If there's still missing values, fill with 0
            df_clean['waterdepth'] = df_clean['waterdepth'].fillna(0)
        
        # Remove any remaining rows with missing values
        df_clean = df_clean.dropna()
        
        # Check for outliers in rainfall data (values significantly higher than normal)
        if 'rainfall' in df_clean.columns:
            # Define threshold as 3x the 99th percentile
            threshold = 3 * df_clean['rainfall'].quantile(0.99)
            outliers = df_clean[df_clean['rainfall'] > threshold]
            
            if not outliers.empty:
                logger.info(f"Found {len(outliers)} outliers in rainfall data")
                # Cap the outliers at the threshold value
                df_clean.loc[df_clean['rainfall'] > threshold, 'rainfall'] = threshold
        
        return df_clean
        
    def interpolate_time_series(self, df: pd.DataFrame, freq: str = '5min') -> pd.DataFrame:
        """
        Resample the time series to a uniform frequency and interpolate missing values
        
        Args:
            df (pd.DataFrame): Input dataframe
            freq (str): Frequency for resampling (default: '5min')
            
        Returns:
            pd.DataFrame: Resampled and interpolated dataframe
        """
        # Make a copy
        df_resampled = df.copy()
        
        # Ensure timestamp column exists
        if 'timestamp' not in df_resampled.columns:
            if 'clctTime' in df_resampled.columns:
                df_resampled['timestamp'] = pd.to_datetime(df_resampled['clctTime'])
            else:
                raise ValueError("No timestamp column found in data")
        
        # Ensure timestamp is a datetime type
        df_resampled['timestamp'] = pd.to_datetime(df_resampled['timestamp'])
        
        # Get numeric columns
        numeric_cols = df_resampled.select_dtypes(include=[np.number]).columns.tolist()
        
        # For each station, resample separately
        resampled_dfs = []
        for station_code, group in df_resampled.groupby('station_code'):
            # Make a copy of the group
            group_copy = group.copy()
            
            # Check for duplicate timestamps
            has_duplicates = group_copy.duplicated('timestamp').any()
            if has_duplicates:
                logger.warning(f"Found duplicate timestamps for station {station_code}. Handling duplicates...")
                
                # Group by timestamp and aggregate
                # For numeric columns, take the mean
                # For categorical columns, take the first value
                numeric_agg = {col: 'mean' for col in numeric_cols if col in group_copy.columns and col != 'station_code'}
                
                # For categorical columns, take the most common value
                categorical_cols = [col for col in group_copy.columns if col not in numeric_cols and col != 'timestamp']
                categorical_agg = {col: lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0] for col in categorical_cols}
                
                # Combine aggregations
                agg_dict = {**numeric_agg, **categorical_agg}
                
                # Apply aggregation to remove duplicates
                group_copy = group_copy.groupby('timestamp').agg(agg_dict).reset_index()
                
                # Add back station_code
                group_copy['station_code'] = station_code
            
            # Now we should have unique timestamps
            # Set timestamp as index
            group_copy = group_copy.set_index('timestamp')
            
            # Resample numeric columns
            try:
                # Get only numeric columns (excluding station_code which might be numeric)
                numeric_data_cols = [col for col in numeric_cols if col in group_copy.columns and col != 'station_code']
                
                if numeric_data_cols:
                    numeric_data = group_copy[numeric_data_cols].copy()
                    
                    # Resample and interpolate
                    resampled_numeric = numeric_data.resample(freq).asfreq()
                    resampled_numeric = resampled_numeric.interpolate(method='linear')
                    
                    # For categorical columns, forward fill
                    categorical_cols = [col for col in group_copy.columns if col not in numeric_cols]
                    
                    if categorical_cols:
                        categorical_data = group_copy[categorical_cols].copy()
                        resampled_categorical = categorical_data.resample(freq).ffill().bfill()
                        
                        # Combine numeric and categorical
                        resampled_group = pd.concat([resampled_numeric, resampled_categorical], axis=1)
                    else:
                        resampled_group = resampled_numeric
                    
                    # Add station_code as a column if it's not already there
                    if 'station_code' not in resampled_group.columns:
                        resampled_group['station_code'] = station_code
                    
                    resampled_dfs.append(resampled_group)
                else:
                    # If no numeric columns, just resample with forward fill
                    resampled_group = group_copy.resample(freq).ffill().bfill()
                    resampled_dfs.append(resampled_group)
            except Exception as e:
                logger.error(f"Error resampling station {station_code}: {str(e)}")
                # If resampling fails, just use the original group
                resampled_dfs.append(group_copy)
        
        # Combine all resampled dataframes
        if resampled_dfs:
            df_resampled = pd.concat(resampled_dfs)
        
        # Reset the index to get timestamp back as a column
        df_resampled = df_resampled.reset_index()
        
        return df_resampled
        
    # def create_sliding_windows(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    #     """
    #     Create sliding windows from time series data
        
    #     Args:
    #         df (pd.DataFrame): Input dataframe
            
    #     Returns:
    #         Tuple[np.ndarray, np.ndarray]: X (input features) and y (target) arrays
    #     """
    #     # Extract input and target columns
    #     input_cols = ['rainfall']
    #     if 'weather' in df.columns:
    #         input_cols.append('weather')
    #     if 'windspeed' in df.columns:
    #         input_cols.append('windspeed')
            
    #     # Also include station_code as a feature
    #     if 'station_code' in df.columns:
    #         input_cols.append('station_code')
            
    #     X_data = df[input_cols].values
        
    #     if 'waterdepth' not in df.columns:
    #         logger.error("Target column 'waterdepth' not found in data")
    #         raise ValueError("Target column 'waterdepth' not found in data")
            
    #     y_data = df['waterdepth'].values
        
    #     # Create sliding windows
    #     X_windows = []
    #     y_windows = []
        
    #     for i in range(0, len(df) - self.window_size + 1, self.step_size):
    #         X_windows.append(X_data[i:i+self.window_size])
    #         # Target is the water depth at the end of the window
    #         y_windows.append(y_data[i+self.window_size-1])
            
    #     return np.array(X_windows), np.array(y_windows)
        
    def fit(self, X, y=None):
        """
        Fit method (required for sklearn compatibility)
        """
        return self
        
    def transform(self, X, y=None):
        """
        Transform method (required for sklearn compatibility)
        
        Args:
            X: Input data (a list of file paths, a DataFrame, or list of DataFrames)
            y: Target data (optional)
            
        Returns:
            Tuple: Processed X and y data
        """
        # Keep the original y
        original_y = y
        
        # If X is a list of file paths, load the data
        if isinstance(X, list) and all(isinstance(item, str) for item in X):
            dfs = [self.load_data(file_path) for file_path in X]
            df = pd.concat(dfs, ignore_index=True)
        elif isinstance(X, list) and all(isinstance(item, pd.DataFrame) for item in X):
            # List of DataFrames
            df = pd.concat(X, ignore_index=True)
        elif isinstance(X, pd.DataFrame):
            df = X
        elif isinstance(X, np.ndarray):
            # If X is already a processed window, just return it with the provided y or a dummy y
            if len(X.shape) == 3:  # Shape: (n_samples, window_size, n_features)
                # Use the provided y if available, otherwise create a dummy y
                if original_y is None:
                    dummy_y = np.zeros(X.shape[0])
                    return X, dummy_y
                else:
                    return X, original_y
        else:
            raise ValueError("Input must be a DataFrame, a list of file paths, or a list of DataFrames")
            
        # Clean the data
        df_clean = self.clean_data(df)
        
        # Extract target if it exists in the dataframe and wasn't provided separately
        if original_y is None and 'waterdepth' in df_clean.columns:
            extracted_y = pd.to_numeric(df_clean['waterdepth'], errors='coerce').fillna(0).values
            logger.info(f"Extracted target from DataFrame in transform method, shape: {extracted_y.shape}")
        else:
            extracted_y = original_y  # Use the provided y or keep it as None
        
        # Interpolate
        df_interpolated = self.interpolate_time_series(df_clean)
        
        # Create sliding windows
        X_windows, sliding_y = self.create_sliding_windows(df_interpolated)
        
        # If y was provided or extracted, and has the right length, use it instead
        if extracted_y is not None and len(extracted_y) == len(sliding_y):
            sliding_y = extracted_y
        
        return X_windows, sliding_y


================================================
File: models/feature_engineer.py
================================================
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
import calendar
import math
from typing import List, Dict, Union, Tuple
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FeatureEngineer(BaseEstimator, TransformerMixin):
    """
    Feature engineering class for waterlogging prediction
    
    Extracts and constructs features from rainfall time series data:
    1. Unit rainfall
    2. Seasonality coefficients
    3. Rainfall interval features
    4. Statistical features
    """
    
    def __init__(self):
        """
        Initialize the FeatureEngineer
        """
        # Define rainy, transition, and dry months based on climatology
        # These can be customized based on local climate
        self.rainy_months = [6, 7, 8, 9]  # June to September
        self.transition_months = [3, 4, 5, 10]  # March to May, October
        self.dry_months = [1, 2, 11, 12]  # November to February
        
        # Define seasonality coefficients
        self.season_coeffs = {
            'rainy': 10,      # Strong coefficient for rainy season
            'transition': 6,  # Medium coefficient for transition season
            'dry': 2          # Low coefficient for dry season
        }
        
        # Feature names
        self.feature_names = [
            'unit_rainfall',
            'seasonality_coeff',
            'rainfall_interval',
            'wetting_coeff',
            'infiltration_capacity',
            'rainfall_mean',
            'rainfall_max',
            'rainfall_std',
            'rainfall_kurtosis',
            'rainfall_skewness',
            'rainfall_auc'
        ]
        
    def get_season_coefficient(self, month: int) -> float:
        """
        Determine the seasonality coefficient based on the month
        
        Args:
            month (int): Month (1-12)
            
        Returns:
            float: Seasonality coefficient
        """
        if month in self.rainy_months:
            return self.season_coeffs['rainy']
        elif month in self.transition_months:
            return self.season_coeffs['transition']
        else:
            return self.season_coeffs['dry']
    
    def calculate_unit_rainfall(self, rainfall_series: np.ndarray) -> np.ndarray:
        """
        Calculate unit rainfall from cumulative rainfall
        
        Args:
            rainfall_series (np.ndarray): Array of rainfall values
            
        Returns:
            np.ndarray: Unit rainfall array
        """
        # Initialize unit rainfall array
        unit_rainfall = np.zeros_like(rainfall_series)
        
        # First value is the same
        unit_rainfall[0] = rainfall_series[0]
        
        # Calculate differences for subsequent values
        for i in range(1, len(rainfall_series)):
            # If current value is less than previous, it's a new rainfall event
            if rainfall_series[i] < rainfall_series[i-1]:
                unit_rainfall[i] = rainfall_series[i]
            else:
                # Calculate increment
                unit_rainfall[i] = rainfall_series[i] - rainfall_series[i-1]
                
        return unit_rainfall
    
    def calculate_rainfall_interval(self, rainfall_series: np.ndarray, threshold: float = 0.1) -> Tuple[np.ndarray, List[int]]:
        """
        Calculate intervals between rainfall events
        
        Args:
            rainfall_series (np.ndarray): Array of unit rainfall values
            threshold (float): Minimum rainfall to consider as an event
            
        Returns:
            Tuple[np.ndarray, List[int]]: Rainfall intervals and event indices
        """
        # Find indices where rainfall exceeds threshold (start of events)
        event_indices = [i for i, r in enumerate(rainfall_series) if r > threshold]
        
        # Calculate intervals between events
        intervals = np.zeros_like(rainfall_series)
        
        if not event_indices:
            return intervals, []
            
        # For indices before first event, set a large interval
        intervals[:event_indices[0]] = 24  # Assuming 24 hours
        
        # Calculate intervals between events
        for i in range(len(event_indices) - 1):
            current_event = event_indices[i]
            next_event = event_indices[i+1]
            interval = next_event - current_event
            
            # Set the interval for all points between these events
            intervals[current_event:next_event] = interval
            
        # For indices after the last event, set the same interval as the last event
        if event_indices:
            intervals[event_indices[-1]:] = intervals[event_indices[-1] - 1] if event_indices[-1] > 0 else 24
            
        return intervals, event_indices
        
    def calculate_wetting_coefficient(self, rainfall_series: np.ndarray, intervals: np.ndarray) -> np.ndarray:
        """
        Calculate wetting coefficient (ratio of mean rainfall to interval)
        
        Args:
            rainfall_series (np.ndarray): Array of rainfall values
            intervals (np.ndarray): Array of rainfall intervals
            
        Returns:
            np.ndarray: Wetting coefficient array
        """
        # Calculate mean rainfall for each point (cumulative mean up to this point)
        cumulative_mean = np.zeros_like(rainfall_series)
        for i in range(len(rainfall_series)):
            if i == 0:
                cumulative_mean[i] = rainfall_series[i]
            else:
                cumulative_mean[i] = np.mean(rainfall_series[:i+1])
        
        # Calculate wetting coefficient
        wetting_coeff = np.zeros_like(rainfall_series)
        
        # Avoid division by zero
        non_zero_intervals = intervals.copy()
        non_zero_intervals[non_zero_intervals == 0] = 1
        
        wetting_coeff = cumulative_mean / non_zero_intervals
        
        return wetting_coeff
        
    def calculate_infiltration_capacity(self, rainfall_series: np.ndarray, intervals: np.ndarray, 
                                        event_indices: List[int]) -> np.ndarray:
        """
        Calculate integrated infiltration capacity
        
        Args:
            rainfall_series (np.ndarray): Array of rainfall values
            intervals (np.ndarray): Array of rainfall intervals
            event_indices (List[int]): Indices of rainfall events
            
        Returns:
            np.ndarray: Infiltration capacity array
        """
        infiltration_capacity = np.zeros_like(rainfall_series)
        
        if not event_indices:
            return infiltration_capacity
            
        # For each event, calculate infiltration capacity
        for i, event_index in enumerate(event_indices):
            # Get the interval for this event
            interval = intervals[event_index]
            
            # Calculate max rainfall for this event
            if i < len(event_indices) - 1:
                event_rainfall = rainfall_series[event_index:event_indices[i+1]]
            else:
                event_rainfall = rainfall_series[event_index:]
                
            if len(event_rainfall) == 0:
                continue
                
            max_rainfall = np.max(event_rainfall)
            
            # Calculate slopes
            slopes = np.zeros_like(event_rainfall)
            for j in range(1, len(event_rainfall)):
                slopes[j] = event_rainfall[j] - event_rainfall[j-1]
                
            # Calculate infiltration capacity
            # Using the formula C_i = e^(-lg(δ)) * R_max * ln(Σ|α|/L)
            if interval > 0 and max_rainfall > 0:
                sum_abs_slopes = np.sum(np.abs(slopes))
                if sum_abs_slopes > 0:
                    term = sum_abs_slopes / len(event_rainfall)
                    if term > 0:
                        capacity = np.exp(-np.log10(interval)) * max_rainfall * np.log(term)
                        
                        # Assign this capacity to all points in this event
                        if i < len(event_indices) - 1:
                            infiltration_capacity[event_index:event_indices[i+1]] = capacity
                        else:
                            infiltration_capacity[event_index:] = capacity
        
        return infiltration_capacity
        
    def calculate_statistical_features(self, rainfall_series: np.ndarray) -> Tuple[float, float, float, float, float, float]:
        """
        Calculate statistical features from rainfall time series
        
        Args:
            rainfall_series (np.ndarray): Array of rainfall values
            
        Returns:
            Tuple[float, float, float, float, float, float]: 
                Mean, Max, Std, Kurtosis, Skewness, AUC
        """
        # Simple statistics
        mean = np.mean(rainfall_series)
        max_val = np.max(rainfall_series)
        std = np.std(rainfall_series)
        
        # Kurtosis
        if std > 0 and len(rainfall_series) > 3:
            n = len(rainfall_series)
            m4 = np.sum((rainfall_series - mean) ** 4) / n
            kurtosis = m4 / (std ** 4) - 3
        else:
            kurtosis = 0
            
        # Skewness
        if std > 0 and len(rainfall_series) > 2:
            n = len(rainfall_series)
            m3 = np.sum((rainfall_series - mean) ** 3) / n
            skewness = m3 / (std ** 3)
        else:
            skewness = 0
            
        # Area Under Curve (simple sum for discrete time series)
        auc = np.sum(rainfall_series)
        
        return mean, max_val, std, kurtosis, skewness, auc
        
    def extract_features_from_window(self, window: np.ndarray, timestamp: pd.Timestamp) -> np.ndarray:
        """
        Extract all features from a rainfall time window
        
        Args:
            window (np.ndarray): Window of rainfall data
            timestamp (pd.Timestamp): Timestamp for the window
            
        Returns:
            np.ndarray: Array of extracted features
        """
        # Get rainfall series from the window
        rainfall_series = window[:, 0]  # Assuming rainfall is the first column
        
        # Get month for seasonality
        month = timestamp.month
        seasonality_coeff = self.get_season_coefficient(month)
        
        # Calculate unit rainfall
        unit_rainfall = self.calculate_unit_rainfall(rainfall_series)
        
        # Calculate rainfall intervals
        intervals, event_indices = self.calculate_rainfall_interval(unit_rainfall)
        
        # Calculate wetting coefficient
        wetting_coeff = self.calculate_wetting_coefficient(rainfall_series, intervals)
        
        # Calculate infiltration capacity
        infiltration_capacity = self.calculate_infiltration_capacity(rainfall_series, intervals, event_indices)
        
        # Calculate statistical features
        mean, max_val, std, kurtosis, skewness, auc = self.calculate_statistical_features(rainfall_series)
        
        # Create feature array
        features = np.array([
            np.mean(unit_rainfall),
            seasonality_coeff,
            np.mean(intervals),
            np.mean(wetting_coeff),
            np.mean(infiltration_capacity),
            mean,
            max_val,
            std,
            kurtosis,
            skewness,
            auc
        ])
        
        return features
        
    def fit(self, X, y=None):
        """
        Fit method (required for sklearn compatibility)
        """
        return self
        
    def transform(self, X, y=None):
        """
        Transform method (required for sklearn compatibility)
        
        Args:
            X: Input data (a tuple from DataProcessor or raw windows)
            y: Target data (optional)
            
        Returns:
            Tuple: Processed X features and y targets
        """
        # Handle different input types
        X_windows = None
        y_windows = None
        timestamps = None
        
        # If X is a tuple from DataProcessor
        if isinstance(X, tuple) and len(X) >= 2:
            X_windows, y_windows = X[:2]
            timestamps = X[2] if len(X) > 2 else [pd.Timestamp.now()] * len(X_windows)
        # If X is raw window data (numpy array)
        elif isinstance(X, np.ndarray):
            if len(X.shape) == 3:  # Expected 3D array (samples, window, features)
                X_windows = X
            elif len(X.shape) == 2:  # Handle 2D array - reshape to 3D
                # Assume it's a single window or needs reshaping
                logger.warning(f"Received 2D array with shape {X.shape}, reshaping to 3D")
                try:
                    # Try to reshape: assume first dimension is samples, second is features
                    # For a single window, create a window size of 1
                    X_windows = X.reshape(X.shape[0], 1, -1)
                except Exception as e:
                    logger.error(f"Failed to reshape input: {str(e)}")
                    # Fallback: create a minimal valid structure
                    X_windows = np.zeros((X.shape[0], 1, 1))
            else:  # Handle other dimensions
                logger.warning(f"Unexpected array shape: {X.shape}, creating dummy structure")
                # Create a minimal valid structure
                X_windows = np.zeros((1, 1, 1))
                
            # Use provided y if available, otherwise create dummy
            if y is not None:
                y_windows = y
            else:
                y_windows = np.zeros(X_windows.shape[0])
            timestamps = [pd.Timestamp.now()] * len(X_windows)
        else:
            # Try to convert to array as last resort
            try:
                logger.warning(f"Received unexpected input type: {type(X)}, attempting conversion")
                X_arr = np.array(X)
                if len(X_arr.shape) >= 2:
                    X_windows = X_arr.reshape(X_arr.shape[0], 1, -1)
                    y_windows = np.zeros(X_arr.shape[0])
                    timestamps = [pd.Timestamp.now()] * X_arr.shape[0]
                else:
                    raise ValueError("Cannot convert input to appropriate format")
            except Exception as e:
                logger.error(f"Conversion failed: {str(e)}")
                raise ValueError("Input must be a tuple from DataProcessor or array data that can be reshaped")
        
        # Make sure X_windows and y_windows are not None
        if X_windows is None or y_windows is None:
            raise ValueError("Failed to process input data")
            
        # Initialize feature array - with safer dimension handling
        n_samples = len(X_windows)
        n_features = len(self.feature_names)
        
        # Determine number of additional features safely
        additional_features = 0
        if len(X_windows.shape) >= 3:
            additional_features = X_windows.shape[2] - 1  # Subtract rainfall
            if additional_features < 0:  # In case only one or zero features
                additional_features = 0
        
        # Total features = extracted rainfall features + additional features
        X_features = np.zeros((n_samples, n_features + additional_features))
        
        # Rest of the method remains the same...


================================================
File: models/performance_analytics.py
================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.gridspec import GridSpec
from mpl_toolkits.mplot3d import Axes3D

# Set the style for the plots
plt.style.use('ggplot')
sns.set(style="whitegrid")

# Define custom colors for each model
colors = {
    'Random Forest': '#3498db',
    'AdaBoost': '#e74c3c',
    'GBDT': '#2ecc71'
}

# Create dummy data for model performance metrics
np.random.seed(42)  # For reproducibility

# Stations
stations = ['Station A', 'Station B', 'Station C']
models = ['Random Forest', 'AdaBoost', 'GBDT']

# Create a DataFrame with model performance metrics
data = {
    'Station': [],
    'Model': [],
    'RMSE': [],
    'R^2': [],
    'Training Time (s)': [],
    'Prediction Time (ms)': []
}

# Fill with realistic data
for station in stations:
    # Base values that vary by station
    if station == 'Station A':
        base_rmse = 0.15
        base_r2 = 0.85
        base_train = 5.0
    elif station == 'Station B':
        base_rmse = 0.12
        base_r2 = 0.88
        base_train = 4.5
    else:  # Station C
        base_rmse = 0.18
        base_r2 = 0.82
        base_train = 5.5
    
    for model in models:
        # Add variation by model
        if model == 'Random Forest':
            rmse = base_rmse * np.random.uniform(0.9, 1.1)
            r2 = base_r2 * np.random.uniform(0.95, 1.02)
            train_time = base_train * np.random.uniform(0.8, 1.2)
            pred_time = 2.5 * np.random.uniform(0.9, 1.1)
        elif model == 'AdaBoost':
            rmse = base_rmse * np.random.uniform(1.1, 1.3)
            r2 = base_r2 * np.random.uniform(0.9, 0.98)
            train_time = base_train * np.random.uniform(0.6, 0.9)
            pred_time = 1.8 * np.random.uniform(0.9, 1.1)
        else:  # GBDT
            rmse = base_rmse * np.random.uniform(0.8, 0.95)
            r2 = base_r2 * np.random.uniform(1.0, 1.05)
            train_time = base_train * np.random.uniform(1.1, 1.4)
            pred_time = 2.0 * np.random.uniform(0.9, 1.1)
        
        # Ensure R² doesn't exceed 1.0
        r2 = min(r2, 0.99)
        
        # Add to data dictionary
        data['Station'].append(station)
        data['Model'].append(model)
        data['RMSE'].append(rmse)
        data['R^2'].append(r2)
        data['Training Time (s)'].append(train_time)
        data['Prediction Time (ms)'].append(pred_time)

# Convert to DataFrame
df = pd.DataFrame(data)

# Calculate overall average performance for each model
# Fix: Only compute mean for numeric columns
model_avg = df.groupby('Model')[['RMSE', 'R^2', 'Training Time (s)', 'Prediction Time (ms)']].mean()

# Print the average performance
print("Average Performance Metrics by Model:")
print(model_avg)

# 1. RMSE Comparison (Bar Chart)
plt.figure(figsize=(14, 10))

# Create a grid for the plots
gs = GridSpec(2, 2, width_ratios=[2, 1])

ax1 = plt.subplot(gs[0, 0])
ax1.set_title('RMSE by Station and Model (lower is better)', fontsize=14, fontweight='bold')

# Create grouped bar chart
sns.barplot(x='Station', y='RMSE', hue='Model', data=df, palette=colors, ax=ax1)
ax1.set_ylabel('Root Mean Squared Error', fontsize=12)
ax1.set_xlabel('Monitoring Station', fontsize=12)
ax1.legend(title='Model')

# Add value labels on bars
for container in ax1.containers:
    ax1.bar_label(container, fmt='%.3f')

# 2. R² Comparison (Bar Chart)
ax2 = plt.subplot(gs[0, 1])
ax2.set_title('R² Score by Model (higher is better)', fontsize=14, fontweight='bold')

# Fixed barplot with hue parameter
sns.barplot(x='R^2', y=model_avg.index, hue=model_avg.index, data=model_avg.reset_index(), 
            palette=colors, ax=ax2, legend=False)
ax2.set_xlabel('Average R² Score', fontsize=12)
ax2.set_ylabel('Model', fontsize=12)

# Add value labels on bars
for container in ax2.containers:
    ax2.bar_label(container, fmt='%.3f')

# 3. Combined Performance Matrix (Heatmap)
ax3 = plt.subplot(gs[1, :])
ax3.set_title('Performance Metrics by Station and Model', fontsize=14, fontweight='bold')

# Pivot the data for the heatmap - normalize metrics to 0-1 range for comparison
pivot_rmse = df.pivot_table(values='RMSE', index='Model', columns='Station')
pivot_r2 = df.pivot_table(values='R^2', index='Model', columns='Station')

# Normalize RMSE (lower is better, so invert)
rmse_min = pivot_rmse.min().min()
rmse_max = pivot_rmse.max().max()
norm_rmse = 1 - (pivot_rmse - rmse_min) / (rmse_max - rmse_min)

# Normalize R² (higher is better)
r2_min = pivot_r2.min().min()
r2_max = pivot_r2.max().max()
norm_r2 = (pivot_r2 - r2_min) / (r2_max - r2_min)

# Calculate combined score (average of normalized metrics)
combined_score = (norm_rmse + norm_r2) / 2

# Create heatmap
sns.heatmap(combined_score, annot=True, cmap='YlGnBu', fmt='.3f', ax=ax3, cbar_kws={'label': 'Combined Performance Score (higher is better)'})
ax3.set_ylabel('Model', fontsize=12)
ax3.set_xlabel('Station', fontsize=12)

plt.tight_layout()
plt.savefig('model_comparison_part1.png', dpi=300, bbox_inches='tight')

# Create a second figure with more visualizations
plt.figure(figsize=(14, 12))

gs2 = GridSpec(2, 2)

# 4. Model Comparison by Station (Line plot)
ax4 = plt.subplot(gs2[0, 0])
ax4.set_title('RMSE by Station for Each Model', fontsize=14, fontweight='bold')

# Create connected line plot
for model in models:
    model_data = df[df['Model'] == model]
    ax4.plot(model_data['Station'], model_data['RMSE'], 'o-', label=model, color=colors[model], linewidth=2, markersize=8)

ax4.set_ylabel('RMSE (lower is better)', fontsize=12)
ax4.set_xlabel('Station', fontsize=12)
ax4.legend(title='Model')

# 5. Training Time vs. R² Scatter Plot
ax5 = plt.subplot(gs2[0, 1])
ax5.set_title('Training Time vs. R² Score', fontsize=14, fontweight='bold')

for model in models:
    model_data = df[df['Model'] == model]
    ax5.scatter(model_data['Training Time (s)'], model_data['R^2'], label=model, color=colors[model], s=100, alpha=0.7)

ax5.set_ylabel('R² Score (higher is better)', fontsize=12)
ax5.set_xlabel('Training Time (seconds)', fontsize=12)
ax5.legend(title='Model')

# 6. 3D Performance Visualization
ax6 = plt.subplot(gs2[1, :], projection='3d')
ax6.set_title('3D Performance Visualization (RMSE, R², Training Time)', fontsize=14, fontweight='bold')

for model in models:
    model_data = df[df['Model'] == model]
    ax6.scatter(model_data['RMSE'], model_data['R^2'], model_data['Training Time (s)'], 
                label=model, color=colors[model], s=100, alpha=0.7)
    
    # Add connecting lines for the same model across stations
    for station in stations:
        station_data = model_data[model_data['Station'] == station]
        ax6.text(station_data['RMSE'].values[0], station_data['R^2'].values[0], 
                 station_data['Training Time (s)'].values[0], station, fontsize=8)

ax6.set_xlabel('RMSE (lower is better)', fontsize=10)
ax6.set_ylabel('R² Score (higher is better)', fontsize=10)
ax6.set_zlabel('Training Time (seconds)', fontsize=10)
ax6.legend(title='Model')

# Add a better angle for 3D visualization
ax6.view_init(elev=30, azim=45)

plt.tight_layout()
plt.savefig('model_comparison_part2.png', dpi=300, bbox_inches='tight')

# Create a third figure for model selection guidance
plt.figure(figsize=(12, 10))

# 7. Performance metrics radar chart
ax7 = plt.subplot(111, polar=True)
ax7.set_title('Model Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)

# Define the metrics and angles
metrics = ['RMSE (inverted)', 'R²', 'Training Speed', 'Prediction Speed']
num_metrics = len(metrics)
angles = np.linspace(0, 2*np.pi, num_metrics, endpoint=False).tolist()
angles += angles[:1]  # Close the loop

# Prepare the data
# Normalize all metrics to 0-1 range where 1 is always better
model_metrics = {}
for model in models:
    # Fix: Select only numeric columns when computing means
    model_data = df[df['Model'] == model][['RMSE', 'R^2', 'Training Time (s)', 'Prediction Time (ms)']].mean()
    
    # RMSE - invert and normalize
    rmse_norm = 1 - (model_data['RMSE'] - df['RMSE'].min()) / (df['RMSE'].max() - df['RMSE'].min())
    
    # R² - normalize
    r2_norm = (model_data['R^2'] - df['R^2'].min()) / (df['R^2'].max() - df['R^2'].min())
    
    # Training speed - invert and normalize (lower time is better)
    train_norm = 1 - (model_data['Training Time (s)'] - df['Training Time (s)'].min()) / (df['Training Time (s)'].max() - df['Training Time (s)'].min())
    
    # Prediction speed - invert and normalize (lower time is better)
    pred_norm = 1 - (model_data['Prediction Time (ms)'] - df['Prediction Time (ms)'].min()) / (df['Prediction Time (ms)'].max() - df['Prediction Time (ms)'].min())
    
    # Store normalized values
    model_metrics[model] = [rmse_norm, r2_norm, train_norm, pred_norm]
    
    # Close the loop for plotting
    model_metrics[model] += model_metrics[model][:1]

# Plot each model
for model, values in model_metrics.items():
    ax7.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[model])
    ax7.fill(angles, values, alpha=0.1, color=colors[model])

# Set the angle labels
ax7.set_xticks(angles[:-1])
ax7.set_xticklabels(metrics)

# Set the y-axis limits
ax7.set_ylim(0, 1)
ax7.grid(True)

# Add legend
plt.legend(loc='upper right', bbox_to_anchor=(0.2, 0.1))

# Add a summary text box
summary_text = """
Model Selection Guidance:

1. Random Forest: Balanced performance with good accuracy and reasonable training time.
   Best for: General use cases where model interpretability is important.

2. AdaBoost: Fastest training but lower accuracy.
   Best for: Scenarios with limited computational resources or time constraints.

3. GBDT: Highest accuracy but longer training time.
   Best for: Applications where prediction accuracy is paramount.
"""

plt.figtext(0.5, 0.02, summary_text, ha="center", fontsize=12, 
            bbox={"facecolor":"white", "alpha":0.5, "pad":5})

plt.tight_layout()
plt.savefig('model_selection_guidance.png', dpi=300, bbox_inches='tight')

# Final conclusion and recommendations
print("\nModel Selection Recommendations:")
print("--------------------------------")
print("1. For high accuracy requirements: GBDT")
print("   - Best overall accuracy (lowest RMSE, highest R²)")
print("   - Consider the longer training time trade-off")
print()
print("2. For balanced performance: Random Forest")
print("   - Good accuracy with reasonable training time")
print("   - More interpretable than other models")
print() 
print("3. For speed-critical applications: AdaBoost")
print("   - Fastest training time")
print("   - Acceptable accuracy for most scenarios")


================================================
File: models/risk_config.joblib
================================================
[Non-text file]


================================================
File: models/risk_predictor.py
================================================
import pandas as pd
import numpy as np
from typing import Dict, List, Union, Optional
import logging
import joblib
import os

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RiskPredictor:
    """
    Risk prediction module for waterlogging based on amplification factors and geospatial features
    """
    
    def __init__(self, weights: Optional[Dict[str, float]] = None, config_path: Optional[str] = None):
        """
        Initialize the RiskPredictor
        
        Args:
            weights (Dict, optional): Weights for different risk factors
            config_path (str, optional): Path to config file with weights and station data
        """
        # Default weights for risk factors
        self.default_weights = {
            'amplification_factor': 0.4,
            'elevation': 0.2,
            'impervious_cover': 0.1,
            'drainage': 0.15,
            'slope': 0.1,
            'proximity_to_water': 0.05
        }
        
        # Use provided weights or defaults
        self.weights = weights or self.default_weights
        
        # Normalize weights to sum to 1
        weight_sum = sum(self.weights.values())
        self.weights = {k: v / weight_sum for k, v in self.weights.items()}
        
        # Station data (will contain amplification factors and geospatial features)
        self.station_data = {}
        
        # Load config if provided
        if config_path and os.path.exists(config_path):
            self.load_config(config_path)
        else:
            # Initialize with dummy data for the stations
            self._initialize_dummy_station_data()
            
    def _initialize_dummy_station_data(self):
        """
        Initialize dummy station data for testing purposes
        """
        # For each station (1-7), create dummy geospatial data
        for station_id in range(1, 8):
            # Random values for each feature, with realistic ranges
            self.station_data[str(station_id)] = {
                'amplification_factor': np.random.uniform(0.1, 0.5),
                'elevation': np.random.uniform(5, 50),  # meters
                'impervious_cover': np.random.uniform(0.6, 0.95),  # fraction
                'drainage_area': np.random.uniform(100, 350),  # m²
                'drainage_volume': np.random.uniform(5000, 15000),  # m³
                'slope': np.random.uniform(0.01, 0.1),  # ratio
                'proximity_to_water': np.random.uniform(0, 500)  # meters
            }
        
        # Normalize geographic features
        self._normalize_features()
        
    def _normalize_features(self):
        """
        Normalize geographic features across all stations to [0,1] range
        """
        # Features to normalize
        features = ['elevation', 'drainage_area', 'drainage_volume', 'slope', 'proximity_to_water']
        
        # Find min and max for each feature
        feature_ranges = {}
        for feature in features:
            values = [station_data[feature] for station_data in self.station_data.values() 
                      if feature in station_data]
            if values:
                feature_ranges[feature] = (min(values), max(values))
        
        # Normalize each feature
        for station_id, data in self.station_data.items():
            for feature in features:
                if feature in data and feature in feature_ranges:
                    min_val, max_val = feature_ranges[feature]
                    if max_val > min_val:  # Avoid division by zero
                        normalized_value = (data[feature] - min_val) / (max_val - min_val)
                        
                        # Store both raw and normalized values
                        data[f'{feature}_normalized'] = normalized_value
        
    def load_config(self, config_path: str):
        """
        Load configuration data (weights and station data) from file
        
        Args:
            config_path (str): Path to the config file
        """
        logger.info(f"Loading risk predictor config from {config_path}")
        
        try:
            config_data = joblib.load(config_path)
            
            # Update weights if provided
            if 'weights' in config_data:
                self.weights = config_data['weights']
                
            # Update station data if provided
            if 'station_data' in config_data:
                self.station_data = config_data['station_data']
                
            # Normalize features
            self._normalize_features()
                
        except Exception as e:
            logger.error(f"Error loading config: {str(e)}")
            # Fall back to dummy data
            self._initialize_dummy_station_data()
            
    def save_config(self, config_path: str):
        """
        Save configuration data to file
        
        Args:
            config_path (str): Path to save the config file
        """
        logger.info(f"Saving risk predictor config to {config_path}")
        
        config_data = {
            'weights': self.weights,
            'station_data': self.station_data
        }
        
        try:
            joblib.dump(config_data, config_path)
        except Exception as e:
            logger.error(f"Error saving config: {str(e)}")
            
    def calculate_amplification_factor(self, station_id: str, rainfall: float, 
                                       waterdepth: float) -> float:
        """
        Calculate the amplification factor (ratio of waterlogging depth to rainfall)
        
        Args:
            station_id (str): Station ID
            rainfall (float): Rainfall amount (mm)
            waterdepth (float): Waterlogging depth (m)
            
        Returns:
            float: Amplification factor
        """
        # Ensure rainfall is not zero to avoid division by zero
        if rainfall <= 0:
            return 0
            
        # Convert rainfall from mm to m for consistent units
        rainfall_m = rainfall / 1000.0
        
        # Calculate amplification factor
        af = waterdepth / rainfall_m
        
        # Update station data
        if station_id in self.station_data:
            # Use exponential moving average to update the factor (smoothing)
            alpha = 0.3  # Smoothing factor
            old_af = self.station_data[station_id].get('amplification_factor', af)
            updated_af = alpha * af + (1 - alpha) * old_af
            self.station_data[station_id]['amplification_factor'] = updated_af
        else:
            # Initialize if station not in data
            self.station_data[station_id] = {'amplification_factor': af}
            
        return af
        
    def predict_risk(self, station_id: str, rainfall: float, waterdepth: float) -> Dict:
        """
        Predict the risk level based on amplification factor and geospatial features
        
        Args:
            station_id (str): Station ID
            rainfall (float): Rainfall amount (mm)
            waterdepth (float): Waterlogging depth (m)
            
        Returns:
            Dict: Risk assessment including level and score
        """
        # Calculate amplification factor
        af = self.calculate_amplification_factor(station_id, rainfall, waterdepth)
        
        # Check if station exists in data
        if station_id not in self.station_data:
            logger.warning(f"Station {station_id} not found in data. Using default values.")
            # Add station with default values if not found
            self.station_data[station_id] = {
                'amplification_factor': af,
                'elevation_normalized': 0.5,
                'impervious_cover': 0.8,
                'drainage_area_normalized': 0.5,
                'drainage_volume_normalized': 0.5,
                'slope_normalized': 0.5,
                'proximity_to_water_normalized': 0.5
            }
        
        # Get station data
        station_data = self.station_data[station_id]
        
        # Calculate risk score
        risk_score = (
            self.weights['amplification_factor'] * station_data.get('amplification_factor', 0) +
            self.weights['elevation'] * (1 - station_data.get('elevation_normalized', 0.5)) +
            self.weights['impervious_cover'] * station_data.get('impervious_cover', 0.8) +
            self.weights['drainage'] * (1 - station_data.get('drainage_volume_normalized', 0.5)) +
            self.weights['slope'] * (1 - station_data.get('slope_normalized', 0.5)) +
            self.weights['proximity_to_water'] * station_data.get('proximity_to_water_normalized', 0.5)
        )
        
        # Determine risk level
        if risk_score < 0.3:
            risk_level = 'low'
        elif risk_score < 0.6:
            risk_level = 'moderate'
        else:
            risk_level = 'high'
            
        return {
            'risk_score': risk_score,
            'risk_level': risk_level,
            'amplification_factor': station_data.get('amplification_factor', 0),
            'factors': {
                'elevation': 1 - station_data.get('elevation_normalized', 0.5),
                'impervious_cover': station_data.get('impervious_cover', 0.8),
                'drainage': 1 - station_data.get('drainage_volume_normalized', 0.5),
                'slope': 1 - station_data.get('slope_normalized', 0.5),
                'proximity_to_water': station_data.get('proximity_to_water_normalized', 0.5)
            }
        }
        
    def update_weights(self, new_weights: Dict[str, float]):
        """
        Update the weights for risk factors
        
        Args:
            new_weights (Dict): New weights for risk factors
        """
        # Update weights
        self.weights.update(new_weights)
        
        # Normalize weights to sum to 1
        weight_sum = sum(self.weights.values())
        self.weights = {k: v / weight_sum for k, v in self.weights.items()}
        
        logger.info(f"Updated risk factor weights: {self.weights}")
        
    def update_station_data(self, station_id: str, data: Dict):
        """
        Update geospatial data for a station
        
        Args:
            station_id (str): Station ID
            data (Dict): New geospatial data
        """
        # Check if station exists
        if station_id not in self.station_data:
            self.station_data[station_id] = {}
            
        # Update data
        self.station_data[station_id].update(data)
        
        # Re-normalize features
        self._normalize_features()
        
        logger.info(f"Updated data for station {station_id}")
        
    def get_station_data(self, station_id: str = None) -> Dict:
        """
        Get geospatial data for a station or all stations
        
        Args:
            station_id (str, optional): Station ID
            
        Returns:
            Dict: Station data
        """
        if station_id:
            return self.station_data.get(station_id, {})
        else:
            return self.station_data
            
    def get_weights(self) -> Dict[str, float]:
        """
        Get the current weights for risk factors
        
        Returns:
            Dict: Current weights
        """
        return self.weights


================================================
File: models/waterlogging_model.joblib
================================================
[Non-text file]


================================================
File: models/waterlogging_predictor.py
================================================
import pandas as pd
import numpy as np
import joblib
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from typing import List, Dict, Union, Tuple, Optional
import logging
import os
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

from models.data_processor import DataProcessor
from models.feature_engineer import FeatureEngineer

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class WaterloggingPredictor(BaseEstimator, RegressorMixin):
    """
    Waterlogging prediction model that uses pipelines for data processing,
    feature engineering, and prediction.
    """
    
    def __init__(self, model_type: str = 'rf', window_size: int = 6, 
                 model_params: Optional[Dict] = None, model_path: Optional[str] = None):
        """
        Initialize the WaterloggingPredictor
        
        Args:
            model_type (str): Type of model to use ('rf', 'gbdt', 'adaboost')
            window_size (int): Size of the sliding window for time series
            model_params (Dict, optional): Parameters for the model
            model_path (str, optional): Path to a pre-trained model file
        """
        self.model_type = model_type
        self.window_size = window_size
        self.model_params = model_params or {}
        self.model_path = model_path
        
        # Initialize pipeline components
        self.data_processor = DataProcessor(window_size=window_size)
        self.feature_engineer = FeatureEngineer()
        
        # Initialize model
        self.model = self._initialize_model()
        
        # Create the pipeline
        self.pipeline = self._create_pipeline()
        
        # Load pre-trained model if provided
        if self.model_path and os.path.exists(self.model_path):
            self.load_model(self.model_path)
            
    def _initialize_model(self):
        """
        Initialize the model based on model_type
        
        Returns:
            BaseEstimator: Initialized model
        """
        if self.model_type == 'rf':
            params = {
                'n_estimators': self.model_params.get('n_estimators', 100),
                'max_depth': self.model_params.get('max_depth', None),
                'min_samples_split': self.model_params.get('min_samples_split', 2),
                'random_state': self.model_params.get('random_state', 42)
            }
            return RandomForestRegressor(**params)
            
        elif self.model_type == 'gbdt':
            params = {
                'n_estimators': self.model_params.get('n_estimators', 100),
                'learning_rate': self.model_params.get('learning_rate', 0.1),
                'max_depth': self.model_params.get('max_depth', 3),
                'random_state': self.model_params.get('random_state', 42)
            }
            return GradientBoostingRegressor(**params)
            
        elif self.model_type == 'adaboost':
            params = {
                'n_estimators': self.model_params.get('n_estimators', 50),
                'learning_rate': self.model_params.get('learning_rate', 1.0),
                'random_state': self.model_params.get('random_state', 42)
            }
            return AdaBoostRegressor(**params)
            
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")
            
    class TransformerWrapper(BaseEstimator, TransformerMixin):
        """
        A wrapper around transformers to ensure only X is passed through the pipeline
        """
        def __init__(self, transformer):
            self.transformer = transformer
            
        def fit(self, X, y=None):
            self.transformer.fit(X, y)
            return self
            
        def transform(self, X):
            result = self.transformer.transform(X, None)
            # Return only X, not the (X, y) tuple
            if isinstance(result, tuple) and len(result) > 0:
                return result[0]
            return result

    def _create_pipeline(self):
        """
        Create the machine learning pipeline with proper target variable handling
        
        Returns:
            Pipeline: Scikit-learn pipeline
        """
        return Pipeline([
            ('data_processor', WaterloggingPredictor.TransformerWrapper(self.data_processor)),
            ('feature_engineer', WaterloggingPredictor.TransformerWrapper(self.feature_engineer)),
            ('model', self.model)
        ])
        
    def fit(self, X, y=None):
        """
        Fit the model with proper handling of pipeline data flow
        
        Args:
            X: Input data (list of file paths or DataFrame)
            y: Target data (not used as targets are extracted from X)
            
        Returns:
            self: The fitted model
        """
        logger.info("Fitting waterlogging prediction model...")
        
        # Extract target if needed
        target_y = y
        
        # X can be a list of file paths, a DataFrame, or a list of DataFrames
        if isinstance(X, list):
            if all(isinstance(item, str) for item in X):
                logger.info(f"Training on {len(X)} data files")
            elif all(isinstance(item, pd.DataFrame) for item in X):
                logger.info(f"Training on {len(X)} DataFrames")
                # Convert list of DataFrames to a single DataFrame
                X = pd.concat(X, ignore_index=True)
        elif isinstance(X, pd.DataFrame):
            logger.info(f"Training on DataFrame with {len(X)} rows")
            
            # Extract target from DataFrame if available and not provided separately
            if target_y is None and 'waterdepth' in X.columns:
                # We need to ensure the target is converted to numeric
                target_y = pd.to_numeric(X['waterdepth'], errors='coerce').fillna(0).values
                logger.info(f"Extracted target variable from DataFrame, shape: {target_y.shape}")
        else:
            raise ValueError("X must be a list of file paths, a DataFrame, or a list of DataFrames")
        
        # If we still don't have a target, try to extract it through the data_processor
        if target_y is None:
            try:
                # Process data through the first step to get features and target
                X_processed, extracted_y = self.data_processor.transform(X)
                if extracted_y is not None and len(extracted_y) > 0:
                    target_y = extracted_y
                    logger.info(f"Extracted target through DataProcessor, shape: {target_y.shape}")
                    
                    # Now process through feature engineering
                    X_features, _ = self.feature_engineer.transform((X_processed, target_y))
                    
                    # Train the model directly
                    self.model.fit(X_features, target_y)
                    logger.info("Model training completed via direct pipeline")
                    return self
            except Exception as e:
                logger.warning(f"Could not extract target through direct pipeline: {str(e)}")
        
        # If we still don't have a target, raise an error
        if target_y is None:
            raise ValueError("Could not extract target variable 'waterdepth' from input data")
        
        try:
            # Use the pipeline with the extracted target
            self.pipeline.fit(X, target_y)
            logger.info("Model training completed via pipeline")
        except Exception as e:
            # If pipeline fails, try direct approach
            logger.warning(f"Pipeline fit failed: {str(e)}. Trying direct approach.")
            
            # Process data through each step manually
            X_processed, _ = self.data_processor.transform(X)
            X_features, _ = self.feature_engineer.transform((X_processed, None))
            
            # Ensure X_features is 2D
            if len(X_features.shape) > 2:
                X_features = X_features.reshape(X_features.shape[0], -1)
                
            # Train the model directly
            self.model.fit(X_features, target_y)
            logger.info("Model training completed via direct approach")
        
        return self

    def predict(self, X):
        """
        Make predictions with proper handling of pipeline data flow
        
        Args:
            X: Input data
            
        Returns:
            np.ndarray: Predicted waterlogging depths
        """
        try:
            # Try using the pipeline
            return self.pipeline.predict(X)
        except Exception as e:
            logger.warning(f"Pipeline prediction failed: {str(e)}. Trying direct approach.")
            
            try:
                # Process data through each step manually
                X_processed, _ = self.data_processor.transform(X)
                
                # Log details about X_processed to debug
                logger.info(f"X_processed shape: {X_processed.shape if hasattr(X_processed, 'shape') else 'no shape attribute'}")
                
                # Handle different output formats from data_processor
                if isinstance(X_processed, np.ndarray):
                    if len(X_processed.shape) < 3:
                        # Reshape to 3D if needed
                        logger.warning(f"Reshaping X_processed from {X_processed.shape} to 3D")
                        X_processed = X_processed.reshape(X_processed.shape[0], 1, -1)
                        
                X_features, _ = self.feature_engineer.transform((X_processed, None))
                
                # Ensure X_features is 2D
                if len(X_features.shape) > 2:
                    X_features = X_features.reshape(X_features.shape[0], -1)
                    
                # Make predictions directly
                return self.model.predict(X_features)
            except Exception as e:
                # Last resort: try to create a minimal viable input for the model
                logger.error(f"Direct prediction approach failed: {str(e)}. Attempting minimal prediction.")
                try:
                    # Create a minimal feature vector based on model's expected input
                    if hasattr(self.model, 'n_features_in_'):
                        n_features = self.model.n_features_in_
                    else:
                        n_features = len(self.feature_engineer.feature_names)
                    
                    # Create dummy features (all zeros)
                    X_features = np.zeros((1, n_features))
                    
                    # Make prediction on dummy data
                    logger.warning("Using dummy features for prediction - result may be inaccurate")
                    return self.model.predict(X_features)
                except Exception as e2:
                    logger.error(f"All prediction attempts failed: {str(e2)}")
                    raise ValueError(f"Failed to make prediction: {str(e)}, {str(e2)}")
        
    def evaluate(self, X_test, y_test=None):
        """
        Evaluate the model on test data
        
        Args:
            X_test: Test data
            y_test: Test targets (optional, will be extracted from X_test if not provided)
            
        Returns:
            Dict: Evaluation metrics
        """
        # Extract target if not provided separately
        if y_test is None and isinstance(X_test, pd.DataFrame) and 'waterdepth' in X_test.columns:
            y_test = pd.to_numeric(X_test['waterdepth'], errors='coerce').fillna(0).values
            logger.info(f"Extracted target from test data, shape: {y_test.shape}")
        
        # Make predictions
        y_pred = self.predict(X_test)
        
        # Extract true values if y_test is still None
        if y_test is None:
            # Process the test data through the first two steps of the pipeline
            X_processed = self.pipeline.steps[0][1].transform(X_test)
            _, y_test = self.pipeline.steps[1][1].transform(X_processed)
        
        # Check if we have valid targets
        if y_test is None or len(y_test) == 0:
            logger.error("Could not extract target values for evaluation")
            return {'mse': float('inf'), 'rmse': float('inf'), 'r2': 0.0}
        
        # Ensure predictions and targets have the same length
        if len(y_pred) != len(y_test):
            logger.warning(f"Prediction length ({len(y_pred)}) doesn't match target length ({len(y_test)})")
            # Use the minimum length
            min_len = min(len(y_pred), len(y_test))
            y_pred = y_pred[:min_len]
            y_test = y_test[:min_len]
        
        # Calculate metrics
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        
        metrics = {
            'mse': mse,
            'rmse': rmse,
            'r2': r2
        }
        
        logger.info(f"Model evaluation metrics: {metrics}")
        return metrics
        
    def cross_validate(self, X, n_splits=5):
        """
        Perform cross-validation
        
        Args:
            X: Input data (DataFrame or list of file paths)
            n_splits (int): Number of CV splits
            
        Returns:
            Dict: Cross-validation results
        """
        logger.info(f"Performing {n_splits}-fold cross-validation")
        
        # Handle different input types
        if isinstance(X, list) and all(isinstance(item, str) for item in X):
            # Load data from file paths
            dfs = [self.data_processor.load_data(file_path) for file_path in X]
            df = pd.concat(dfs, ignore_index=True)
        elif isinstance(X, pd.DataFrame):
            df = X
        else:
            raise ValueError("X must be a DataFrame or list of file paths")
        
        # Process the data through data_processor
        X_clean = self.data_processor.clean_data(df)
        
        # Make sure timestamp is datetime
        if 'timestamp' in X_clean.columns:
            X_clean['timestamp'] = pd.to_datetime(X_clean['timestamp'])
        
        # Create train/test splits based on time for temporal data
        if 'timestamp' in X_clean.columns:
            # Sort by timestamp
            X_clean = X_clean.sort_values('timestamp')
            
            # Create time-based folds
            time_indices = np.array_split(np.arange(len(X_clean)), n_splits)
            cv_splits = [(np.concatenate(time_indices[:i] + time_indices[i+1:]), time_indices[i]) 
                        for i in range(n_splits)]
        else:
            # If no timestamp, use KFold
            kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
            cv_splits = list(kf.split(X_clean))
        
        # Store metrics for each fold
        mse_scores = []
        r2_scores = []
        
        # Perform CV
        for i, (train_idx, test_idx) in enumerate(cv_splits):
            try:
                # Split data
                X_train = X_clean.iloc[train_idx].copy()
                X_test = X_clean.iloc[test_idx].copy()
                
                # Train model on this fold
                fold_model = WaterloggingPredictor(
                    model_type=self.model_type,
                    window_size=self.window_size,
                    model_params=self.model_params
                )
                
                # Fit the model
                fold_model.fit(X_train)
                
                # Evaluate
                metrics = fold_model.evaluate(X_test)
                
                mse = metrics['mse']
                r2 = metrics['r2']
                
                mse_scores.append(mse)
                r2_scores.append(r2)
                
                logger.info(f"Fold {i+1}: MSE = {mse:.6f}, R² = {r2:.6f}")
                
            except Exception as e:
                logger.error(f"Error in fold {i+1}: {str(e)}")
                # If a fold fails, use placeholder values
                mse_scores.append(float('inf'))
                r2_scores.append(0.0)
        
        # Calculate average metrics (excluding any failed folds)
        valid_mse = [mse for mse in mse_scores if mse != float('inf')]
        valid_r2 = [r2 for r2 in r2_scores if r2 != 0.0]
        
        avg_mse = np.mean(valid_mse) if valid_mse else float('inf')
        avg_r2 = np.mean(valid_r2) if valid_r2 else 0.0
        
        cv_results = {
            'mse_scores': mse_scores,
            'r2_scores': r2_scores,
            'avg_mse': avg_mse,
            'avg_r2': avg_r2
        }
        
        logger.info(f"Average MSE: {avg_mse:.6f}, Average R²: {avg_r2:.6f}")
        return cv_results
        
    def tune_hyperparameters(self, X, param_grid=None):
        """
        Tune model hyperparameters using GridSearchCV
        
        Args:
            X: Input data
            param_grid (Dict, optional): Parameter grid for search
            
        Returns:
            Dict: Best parameters and results
        """
        logger.info("Tuning model hyperparameters")
        
        # Default parameter grid if not provided
        if param_grid is None:
            if self.model_type == 'rf':
                param_grid = {
                    'model__n_estimators': [50, 100, 200],
                    'model__max_depth': [None, 10, 20],
                    'model__min_samples_split': [2, 5, 10]
                }
            elif self.model_type == 'gbdt':
                param_grid = {
                    'model__n_estimators': [50, 100, 200],
                    'model__learning_rate': [0.01, 0.1, 0.2],
                    'model__max_depth': [3, 5, 7]
                }
            elif self.model_type == 'adaboost':
                param_grid = {
                    'model__n_estimators': [50, 100, 200],
                    'model__learning_rate': [0.01, 0.1, 1.0]
                }
            
        # Create a GridSearchCV object
        grid_search = GridSearchCV(
            self.pipeline,
            param_grid,
            cv=5,
            scoring='neg_mean_squared_error',
            n_jobs=-1,
            verbose=1
        )
        
        # Fit the grid search
        grid_search.fit(X)
        
        # Get the best parameters and score
        best_params = grid_search.best_params_
        best_score = -grid_search.best_score_  # Convert back to MSE
        
        logger.info(f"Best parameters: {best_params}")
        logger.info(f"Best MSE: {best_score:.6f}")
        
        # Update the model with best parameters
        self.model_params = {k.replace('model__', ''): v for k, v in best_params.items()}
        self.model = self._initialize_model()
        self.pipeline = self._create_pipeline()
        
        return {
            'best_params': best_params,
            'best_score': best_score,
            'cv_results': grid_search.cv_results_
        }
        
    def save_model(self, filepath):
        """
        Save the trained model to a file
        
        Args:
            filepath (str): Path to save the model
        """
        logger.info(f"Saving model to {filepath}")
        joblib.dump(self.pipeline, filepath)
        
    def load_model(self, filepath):
        """
        Load a trained model from a file
        
        Args:
            filepath (str): Path to the model file
        """
        if not os.path.exists(filepath):
            logger.error(f"Model file not found: {filepath}")
            raise FileNotFoundError(f"Model file not found: {filepath}")
            
        logger.info(f"Loading model from {filepath}")
        loaded_data = joblib.load(filepath)
        
        # Check if loaded data is a dictionary (our custom format) or a pipeline
        if isinstance(loaded_data, dict):
            # Extract components from dictionary
            if 'data_processor' in loaded_data:
                self.data_processor = loaded_data['data_processor']
            if 'feature_engineer' in loaded_data:
                self.feature_engineer = loaded_data['feature_engineer']
            if 'model' in loaded_data:
                self.model = loaded_data['model']
            if 'model_type' in loaded_data:
                self.model_type = loaded_data['model_type']
            if 'window_size' in loaded_data:
                self.window_size = loaded_data['window_size']
                
            # Recreate the pipeline with the loaded components
            self.pipeline = self._create_pipeline()
        else:
            # Assume it's a pipeline
            self.pipeline = loaded_data
            
            # Extract pipeline components
            self.data_processor = self.pipeline.named_steps['data_processor'].transformer
            self.feature_engineer = self.pipeline.named_steps['feature_engineer'].transformer
            self.model = self.pipeline.named_steps['model']
        
    def update_model(self, X_new, y_new=None):
        """
        Update the model with new data (online learning)
        
        Args:
            X_new: New data
            y_new: New targets (not used as targets are extracted from X_new)
            
        Returns:
            self: The updated model
        """
        logger.info("Updating model with new data")
        
        # Convert input to appropriate format if needed
        if isinstance(X_new, pd.DataFrame):
            # If X_new is a single dataframe
            pass
        elif isinstance(X_new, str):
            # If X_new is a file path
            X_new = [X_new]
        
        # Process new data
        X_processed = self.pipeline.steps[0][1].transform(X_new)
        X_features, y_new_processed = self.pipeline.steps[1][1].transform(X_processed)
        
        # Update model (if model supports partial_fit, use it, otherwise retrain)
        if hasattr(self.model, 'partial_fit'):
            self.model.partial_fit(X_features, y_new_processed)
        else:
            # Get existing data (if available)
            try:
                # This is a simplified approach - in a real system, you might want to
                # store the training data separately or use incremental learning
                X_features_old = getattr(self, '_X_features', None)
                y_old = getattr(self, '_y', None)
                
                if X_features_old is not None and y_old is not None:
                    # Combine old and new data
                    X_features_combined = np.vstack([X_features_old, X_features])
                    y_combined = np.concatenate([y_old, y_new_processed])
                else:
                    X_features_combined = X_features
                    y_combined = y_new_processed
                    
                # Store for future updates
                self._X_features = X_features_combined
                self._y = y_combined
                
                # Retrain the model
                self.model.fit(X_features_combined, y_combined)
            except:
                # If retrieval of old data fails, just train on new data
                self.model.fit(X_features, y_new_processed)
                
        logger.info("Model updated successfully")
        return self
        
    def get_feature_importance(self):
        """
        Get feature importance from the model
        
        Returns:
            Dict: Feature names and their importance scores
        """
        # Check if model has feature_importances_ attribute
        if not hasattr(self.model, 'feature_importances_'):
            logger.warning("Model does not provide feature importances")
            return None
            
        # Get feature names
        feature_names = (
            self.feature_engineer.feature_names + 
            ['additional_feature_' + str(i) for i in range(self.model.feature_importances_.shape[0] - len(self.feature_engineer.feature_names))]
        )
        
        # Create feature importance dictionary
        feature_importance = dict(zip(feature_names, self.model.feature_importances_))
        
        # Sort by importance
        feature_importance = {k: v for k, v in sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)}
        
        return feature_importance



